{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"thesis2.ipynb","provenance":[{"file_id":"1_fl6WRDCnwSSVJPKXp3yPiTwYYd0Y_k3","timestamp":1578883234875}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EchhwRUwN7RD","colab_type":"code","outputId":"8adc56a0-0c9f-4271-ac34-196f42d40124","executionInfo":{"status":"ok","timestamp":1578883372490,"user_tz":-480,"elapsed":22575,"user":{"displayName":"代安語","photoUrl":"","userId":"02683702717166278139"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2P4yvHx1XEFz","colab_type":"code","outputId":"33e2c918-aebf-4051-da7a-037c07b8868c","executionInfo":{"status":"ok","timestamp":1578888700122,"user_tz":-480,"elapsed":2581,"user":{"displayName":"代安語","photoUrl":"","userId":"02683702717166278139"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd '/content/drive/My Drive/thesis'\n","import config_16x15_seq\n","%cd '/content/drive/My Drive/thesis/config_16x15_seq'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/thesis\n","/content/drive/My Drive/thesis/config_16x15_seq\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rs_DcUTNXNNg","colab_type":"code","outputId":"18f0393f-91f8-4492-ac93-ba3356ab2dd7","executionInfo":{"status":"ok","timestamp":1578883431819,"user_tz":-480,"elapsed":4056,"user":{"displayName":"代安語","photoUrl":"","userId":"02683702717166278139"}},"colab":{"base_uri":"https://localhost:8080/","height":97}},"source":["import os\n","import pprint\n","import tensorflow as tf\n","\n","if 'COLAB_TPU_ADDR' not in os.environ:\n","    device_name = tf.test.gpu_device_name()\n","    if device_name != '/device:GPU:0':\n","        print('ERROR!')\n","    else:\n","        print('Found GPU at: {}'.format(device_name))\n","else:\n","    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","    print('TPU address is', tpu_address)\n","    with tf.Session(tpu_address) as session:\n","        devices = session.list_devices()\n","        print('TPU devices:')\n","        pprint.pprint(devices)\n","\n"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wqojsneLXlfE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4c8efe98-8578-4bcf-e906-de62e3b4009d","executionInfo":{"status":"ok","timestamp":1578909946105,"user_tz":-480,"elapsed":21235172,"user":{"displayName":"代安語","photoUrl":"","userId":"02683702717166278139"}}},"source":["from config_16x15_seq.builder import *\n","from tqdm import tqdm\n","from keras.callbacks import BaseLogger, History, CallbackList\n","from scipy.ndimage import gaussian_filter\n","import copy\n","\n","MU_training = MU['training']\n","MU_norm_training = MU_norm['training']\n","PHI_meas_training = PHI_meas['training']\n","MUa_training = MUa['training']\n","MUsp_training = MUsp['training']\n","freq_training = freq['training']\n","d_training = d['training']\n","lr = 0.0002\n","beta_1 = 0.5\n","# clip_value = 0.01\n","optimizer = Adam(lr=lr, beta_1=beta_1)\n","# optimizer = RMSprop(lr=lr)\n","generator = primary_net()\n","print('Generator model summary:')\n","generator.summary()\n","discriminator = secondary_net()\n","discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[custom_binary_accuracy])\n","print('Discriminator model summary:')\n","discriminator.summary()\n","discriminator.trainable = False\n","# inputs = Input((PHI_meas_training.shape[1],))\n","outputs = generator.outputs\n","outputs.append(discriminator(outputs[:2]))\n","gan = Model(inputs=generator.inputs, outputs=outputs)\n","loss = ['mse', 'mse', 'mse', 'mse', 'mse', 'mse', 'binary_crossentropy']\n","loss_weights = [0, 0, 5, 5, 1e4, 1, 1]\n","metrics = ['mae', 'mse', 'mape', 'msle', 'logcosh', 'cosine', custom_binary_accuracy]\n","gan.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer, metrics=metrics)\n","print('Generative adversarial network model summary:')\n","gan.summary()\n","history = History()\n","logger = BaseLogger(stateful_metrics=['discriminator_' + s for s in discriminator.stateful_metric_names] +\n","                                     ['gan_' + s for s in gan.stateful_metric_names])\n","# checkpoint = callbacks.ModelCheckpoint('model_test{epoch:02d}.h5', verbose=1)\n","time_history = TimeHistory()\n","callbacks = [logger, time_history, history]\n","out_labels = ['discriminator_' + s for s in discriminator.metrics_names] + ['gan_' + s for s in gan.metrics_names]\n","callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n","callbacks = CallbackList(callbacks)\n","# callbacks = [TimeHistory(), EarlyStopping(monitor='loss', min_delta=0.1, patience=100,\n","#                                           restore_best_weights=True, verbose=1)]\n","# early_stop = EarlyStopping(patience=200, verbose=1)\n","choice = np.zeros(data_size, dtype='uint8')\n","choice[np.random.choice(data_size, val_size, replace=False)] = 1\n","MU_training_train = MU_training[choice == 0]\n","MU_training_val = MU_training[choice == 1]\n","MU_norm_training_train = MU_norm_training[choice == 0]\n","MU_norm_training_val = MU_norm_training[choice == 1]\n","PHI_meas_training_train = PHI_meas_training[choice == 0]\n","PHI_meas_training_val = PHI_meas_training[choice == 1]\n","freq_training_train = freq_training[choice == 0]\n","freq_training_val = freq_training[choice == 1]\n","d_training_train = d_training[choice == 0]\n","d_training_val = d_training[choice == 1]\n","MUa_train = MUa_training[choice == 0]\n","MUa_val = MUa_training[choice == 1]\n","MUsp_train = MUsp_training[choice == 0]\n","MUsp_val = MUsp_training[choice == 1]\n","# choice = data_cat\n","batch_size = 32\n","prob_param = 20.0\n","prob_param2 = 1.0\n","# label_softness = 0.1\n","# metrics_eval_n = 50\n","# metrics_eval_step = 20\n","hard_valid = np.ones(val_size)\n","hard_fake = np.zeros(val_size)\n","\n","\n","# soft_label = False\n","# augment = True\n","# min_img_loss_avg = np.inf\n","# min_dis_loss_avg = np.inf\n","# img_loss_baseline_factor = 1.1\n","# img_loss_threshold = 1.2\n","# img_loss_threshold2 = 0.6\n","# prob_augment = 0.5\n","# gan_d_acc = [0.5, 0.5]\n","# init_train = 100\n","\n","N_count_train = np.zeros_like(N_count)\n","n = 0\n","for i in range(len(N_count)):\n","    ni = N_count[i]\n","    N_count_train[i] = np.where(choice[n:(n + ni)] == 0)[0].size\n","    n += ni\n","index_array = np.arange(train_size)\n","index_array2 = np.arange(train_size)\n","ia = np.arange(train_size)\n","n = N_count_train[0]\n","np.random.shuffle(index_array[:n])\n","np.random.shuffle(index_array2[:n])\n","for i in range(1, len(N_count_train) - 1):\n","    ni = N_count_train[i]\n","    np.random.shuffle(index_array[n:(n + ni)])\n","    np.random.shuffle(index_array2[n:(n + ni)])\n","    n += ni\n","np.random.shuffle(index_array[n:])\n","np.random.shuffle(index_array2[n:])\n","\n","\n","def run_epoch(epochs):\n","    callbacks.set_params({\n","        'batch_size': batch_size,\n","        'epochs': epochs,\n","        'steps': None,\n","        'samples': train_size,\n","        'verbose': 2,\n","        'do_validation': True,\n","        'metrics': callback_metrics,\n","    })\n","    callbacks.on_train_begin()\n","    for epoch in range(epochs):\n","        for m in discriminator.stateful_metric_functions:\n","            m.reset_states()\n","        for m in gan.stateful_metric_functions:\n","            m.reset_states()\n","        callbacks.on_epoch_begin(epoch)\n","        epoch_logs = {}\n","        progress_bar = None\n","        # np.random.shuffle(ia)\n","        n = N_count_train[0]\n","        np.random.shuffle(ia[:n])\n","        for i in range(1, len(N_count_train) - 1):\n","            ni = N_count_train[i]\n","            np.random.shuffle(ia[n:(n + ni)])\n","            n += ni\n","        np.random.shuffle(ia[n:])\n","        num_batches = (train_size + batch_size - 1) // batch_size\n","        # if epoch > 0 and epoch % metrics_eval_step == 0:\n","        # hist_avg = {}\n","        # for k, v in history.history.items():\n","        #     if k.startswith('val_'):\n","        #         l = k[4:]\n","        #     else:\n","        #         l = k\n","        #     if l in logger.stateful_metrics:\n","        #         hist_avg[k] = v[-1]\n","        #     else:\n","        #         hist_avg[k] = np.average(v[-metrics_eval_n:])\n","        # img_loss_avg = hist_avg['val_gan_' + gan.metrics_names[0]] - hist_avg['val_gan_' + gan.metrics_names[7]]\n","        # dis_loss_avg = hist_avg['val_discriminator_' + discriminator.metrics_names[0]]\n","        # min_img_loss = np.min(history.history['val_gan_' + gan.metrics_names[0]]) \\\n","        #                - np.min(history.history['val_gan_' + gan.metrics_names[7]])\n","        # if img_loss_avg > min_img_loss_avg * img_loss_baseline_factor:\n","        #     if not augment:\n","        #         augment = True\n","        #     elif img_loss_avg >= img_loss_threshold and min_img_loss >= img_loss_threshold2:\n","        #         soft_label = True\n","        # elif dis_loss_avg > min_dis_loss_avg:\n","        #     if augment:\n","        #         augment = False\n","        #     elif img_loss_avg < img_loss_threshold:\n","        #         soft_label = False\n","        # img_loss_avg = np.average(history.history['val_gan_' + gan.metrics_names[0]][-metrics_eval_n:])\\\n","        #                - np.average(history.history['val_gan_' + gan.metrics_names[7]][-metrics_eval_n:])\n","        # if soft_label and img_loss_avg < img_loss_threshold:\n","        #     soft_label = False\n","        # if np.random.random() > 0.5:\n","        #     augment = not augment\n","        # min_img_loss_avg = min(img_loss_avg, min_img_loss_avg)\n","        # min_dis_loss_avg = min(dis_loss_avg, min_dis_loss_avg)\n","        # if epoch >= init_train:\n","        #     if gan_d_acc[0] < 0.5 or gan_d_acc[1] < 0.5:\n","        #         prob_augment = 0.8\n","        #     else:\n","        #         prob_augment = 0.5\n","        #     if augment == (np.random.random() < prob_augment):\n","        #         augment = not augment\n","        batch_end = 0\n","        # prob_augment = np.random.random()\n","        for batch_index in range(num_batches):\n","            augment = np.random.random() < 0.5\n","            batch_start = batch_end\n","            batch_end = min(train_size, batch_end + batch_size)\n","            batch_ids = index_array[ia[batch_start:batch_end]]\n","            batch_logs = {'batch': batch_index, 'size': len(batch_ids)}\n","            callbacks.on_batch_begin(batch_index, batch_logs)\n","            MU_gt = [MU_training_train[batch_ids, 0], MU_training_train[batch_ids, 1]]\n","            MU_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_gt]\n","            in_gen = [PHI_meas_training_train[batch_ids], freq_training_train[batch_ids], d_training_train[batch_ids]]\n","            if augment:\n","                noise_param = np.random.random(len(batch_ids))\n","                blur_param = np.random.random(len(batch_ids))\n","                noise_param *= blur_param\n","                MU_gt_aug = [np.copy(im) for im in MU_gt]\n","                for i, bi in enumerate(batch_ids):\n","                    temp = mask_image(MU_gt_aug[0][i, 0], MUa_train[bi])\n","                    temp = temp + temp * np.random.normal(scale=noise_param[i] * 0.5, size=temp.shape)\n","                    temp = gaussian_filter(temp, blur_param[i] * 5.0)\n","                    MU_gt_aug[0][i, 0] = mask_image(temp, 0.0)\n","                    temp = mask_image(MU_gt_aug[1][i, 0], MUsp_train[bi])\n","                    temp = temp + temp * np.random.normal(scale=noise_param[i] * 0.5, size=temp.shape)\n","                    temp = gaussian_filter(temp, blur_param[i] * 5.0)\n","                    MU_gt_aug[1][i, 0] = mask_image(temp, 0.0)\n","                # temp = np.clip(blur_param, prob_param2 / prob_param, 1.0 - prob_param2 / prob_param)\n","                valid = 0.95 - 0.05 * np.random.random(len(batch_ids))\n","                # valid = 1.0 - label_softness * np.random.beta(temp * prob_param, (1.0 - temp) * prob_param)\n","                # if soft_label:\n","                #     fake = label_softness * np.random.random(len(batch_ids))\n","                #     valid = 1.0 - label_softness * np.random.beta(temp * prob_param, (1.0 - temp) * prob_param)\n","                # else:\n","                #     fake = hard_fake[:len(batch_ids)]\n","                #     valid = hard_valid[:len(batch_ids)] - label_softness\n","            else:\n","                MU_gt_aug = MU_gt\n","                valid = hard_valid[:len(batch_ids)] - 0.05\n","                # valid = 1.0 - 0.05 * np.random.random(len(batch_ids))\n","                # valid = 1.0 - label_softness * np.random.random(len(batch_ids))\n","                # valid = 1.0 - label_softness * np.random.beta(prob_param2,\n","                #                                               prob_param - prob_param2, size=len(batch_ids))\n","                # if soft_label:\n","                #     fake = label_softness * np.random.random(len(batch_ids))\n","                #     valid = 1.0 - label_softness * np.random.beta(\n","                #         prob_param2, prob_param - prob_param2, size=len(batch_ids))\n","                # else:\n","                #     fake = hard_fake[:len(batch_ids)]\n","                #     valid = hard_valid[:len(batch_ids)] - label_softness\n","            # fake = hard_fake[:len(batch_ids)]\n","            fake = 0.05 * np.random.random(len(batch_ids))\n","            # if epoch < init_train:\n","            #     valid -= label_softness * np.random.random(len(batch_ids)) * (init_train - epoch) / init_train\n","            #     fake += label_softness * np.random.random(len(batch_ids)) * (init_train - epoch) / init_train\n","            d_loss1 = discriminator.train_on_batch(MU_gt_aug, valid)\n","            # d_loss1 = discriminator.train_on_batch([np.concatenate([im1, im2], axis=0)\n","            #                                        for im1, im2 in zip(MU_gt_aug, MU_gt)],\n","            #                                        np.concatenate([valid, hard_valid[:len(batch_ids)]], axis=0),\n","            #                                        sample_weight=np.ones(2 * len(batch_ids)) * 0.5)\n","            d_loss2 = discriminator.train_on_batch(generator.predict(in_gen)[:2], fake)\n","            # d_loss = discriminator.train_on_batch([np.concatenate([im1, im2], axis=0)\n","            #                                        for im1, im2 in zip(MU_gt_aug, generator.predict(in_gen)[:2])],\n","            #                                       np.concatenate([valid, fake], axis=0))\n","            d_loss = np.zeros(len(discriminator.metrics_names))\n","            for i, l in enumerate(discriminator.metrics_names):\n","                if l in discriminator.stateful_metric_names:\n","                    d_loss[i] = d_loss2[i]\n","                else:\n","                    d_loss[i] = 0.5 * (d_loss1[i] + d_loss2[i])\n","            # for l in discriminator.layers:\n","            #     weights = l.get_weights()\n","            #     weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n","            #     l.set_weights(weights)\n","            batch_ids = index_array2[ia[batch_start:batch_end]]\n","            MU_gt = [MU_training_train[batch_ids, 0], MU_training_train[batch_ids, 1]]\n","            MU_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_gt]\n","            in_gen = [PHI_meas_training_train[batch_ids], freq_training_train[batch_ids], d_training_train[batch_ids]]\n","            MU_norm_gt = [MU_norm_training_train[batch_ids, 0], MU_norm_training_train[batch_ids, 1]]\n","            MU_norm_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_norm_gt]\n","            g_loss = gan.train_on_batch(in_gen, [MU_gt[0], MU_gt[1], MU_norm_gt[0], MU_norm_gt[1],\n","                                                 MUa_train[batch_ids], MUsp_train[batch_ids],\n","                                                 hard_valid[:len(batch_ids)]])\n","            for l, o in zip(out_labels, np.concatenate([d_loss, g_loss])):\n","                batch_logs[l] = o\n","            callbacks.on_batch_end(batch_index, batch_logs)\n","            if progress_bar is None:\n","                progress_bar = tqdm(total=train_size, desc='Epoch {}/{}'.format(epoch + 1, epochs), unit='samples')\n","            progress_bar.set_postfix(train_on_batch=('[D loss: [%g %g], acc.: [%.2f%% %.2f%%]] [G total_loss: %g, ' +\n","                                                     'img_loss: %g, d_loss: %g, d_acc.: %.2f%%]') %\n","                                                    (d_loss1[0], d_loss2[0], 100 * d_loss1[1], 100 * d_loss2[1],\n","                                                     g_loss[0],\n","                                                     g_loss[0] - g_loss[7], g_loss[7], 100 * g_loss[-1]),\n","                                     augment=augment)\n","            progress_bar.update(len(batch_ids))\n","        MU_gt = [MU_training_val[:, np.array([0])], MU_training_val[:, np.array([1])]]\n","        d_val_loss1 = discriminator.evaluate(MU_gt, hard_valid, batch_size=batch_size, verbose=0)\n","        d_val_loss2 = discriminator.evaluate(generator.predict([PHI_meas_training_val, freq_training_val,\n","                                                                d_training_val])[:2], hard_fake, batch_size=batch_size,\n","                                             verbose=0)\n","        # d_val_loss = discriminator.evaluate([np.concatenate([im1, im2], axis=0)\n","        #                                      for im1, im2 in zip(MU_gt,\n","        #                                                          generator.predict(PHI_meas_training_val)[:2])],\n","        #                                     np.concatenate([hard_valid, hard_fake], axis=0), verbose=0)\n","        d_val_loss = np.zeros(len(discriminator.metrics_names))\n","        for i, l in enumerate(discriminator.metrics_names):\n","            if l in discriminator.stateful_metric_names:\n","                d_val_loss[i] = d_val_loss2[i]\n","            else:\n","                d_val_loss[i] = 0.5 * (d_val_loss1[i] + d_val_loss2[i])\n","        MU_norm_gt = [MU_norm_training_val[:, np.array([0])], MU_norm_training_val[:, np.array([1])]]\n","        progress_bar.set_postfix(validate_on_epoch='[D loss: [%g %g], acc.: [%.2f%% %.2f%%]] validating generator..' %\n","                                                   (d_val_loss1[0], d_val_loss2[0], 100 * d_val_loss1[1],\n","                                                    100 * d_val_loss2[1]))\n","        g_val_loss = gan.evaluate([PHI_meas_training_val, freq_training_val, d_training_val],\n","                                  [MU_gt[0], MU_gt[1], MU_norm_gt[0], MU_norm_gt[1], MUa_val, MUsp_val, hard_valid],\n","                                  batch_size=batch_size, verbose=0)\n","        for l, o in zip(out_labels, np.concatenate([d_val_loss, g_val_loss])):\n","            epoch_logs['val_' + l] = o\n","        # num_batches = (val_size + batch_size - 1) // batch_size\n","        # for batch_index in range(num_batches):\n","        #     batch_start = batch_index * batch_size\n","        #     batch_end = min(val_size, (batch_index + 1) * batch_size)\n","        #     MU_gt = [MU_training_val[batch_start:batch_end, 0], MU_training_val[batch_start:batch_end, 0]]\n","        #     MU_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_gt]\n","        #     d_loss1 = discriminator.test_on_batch(MU_gt, hard_valid[:(batch_end - batch_start)])\n","        #     in_gen = PHI_meas_training_val[batch_start:batch_end]\n","        #     d_loss2 = discriminator.test_on_batch(generator.predict(in_gen)[:2], hard_fake[:(batch_end - batch_start)])\n","        #     d_loss = np.zeros(len(discriminator.metrics_names))\n","        #     for i, l in enumerate(discriminator.metrics_names):\n","        #         if l in discriminator.stateful_metric_names:\n","        #             d_loss[i] = d_loss2[i]\n","        #         else:\n","        #             d_loss[i] = 0.5 * (d_loss1[i] + d_loss2[i])\n","        #     MU_norm_gt = [MU_norm_training_val[batch_start:batch_end, 0],\n","        #                   MU_norm_training_val[batch_start:batch_end, 1]]\n","        #     MU_norm_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_norm_gt]\n","        #     g_loss = gan.test_on_batch(in_gen, [MU_gt[0], MU_gt[1], MU_norm_gt[0], MU_norm_gt[1],\n","        #                                       MUa_val[batch_start:batch_end], MUsp_val[batch_start:batch_end],\n","        #                                       hard_valid[:(batch_end - batch_start)]])\n","        #     for l, o in zip(out_labels, np.concatenate([d_loss, g_loss])):\n","        #         if l in logger.stateful_metrics:\n","        #             epoch_logs['val_' + l] = o\n","        #         else:\n","        #             if 'val_' + l in epoch_logs:\n","        #                 epoch_logs['val_' + l] += o * (batch_end - batch_start)\n","        #             else:\n","        #                 epoch_logs['val_' + l] = o * (batch_end - batch_start)\n","        #     pbar.set_postfix(validate_on_batch=('[D loss: %g, acc.: %.2f%%] [G total_loss: %g, img_loss: %g, ' +\n","        #                                         'd_loss: %g, d_acc.: %.2f%%]') %\n","        #                                        (d_loss[0], 100 * d_loss[1], g_loss[0], g_loss[0] - g_loss[7],\n","        #                                         g_loss[7], 100 * g_loss[-1]), soft_label=soft_label, augment=augment)\n","        #     pbar.update(batch_end - batch_start)\n","        # for l in out_labels:\n","        #     if l not in logger.stateful_metrics:\n","        #         epoch_logs['val_' + l] /= val_size\n","        callbacks.on_epoch_end(epoch, epoch_logs)\n","        # gan_d_acc[0] = epoch_logs[out_labels[-1]]\n","        # gan_d_acc[1] = epoch_logs['val_' + out_labels[-1]]\n","        progress_bar.set_postfix(train=('[D loss: %g, acc.: %.2f%%] [G total_loss: %g, img_loss: %g, d_loss: %g, ' +\n","                                        'd_acc.: %.2f%%]') %\n","                                       (epoch_logs[out_labels[0]], 100 * epoch_logs[out_labels[1]],\n","                                        epoch_logs[out_labels[2]],\n","                                        epoch_logs[out_labels[2]] - epoch_logs[out_labels[9]],\n","                                        epoch_logs[out_labels[9]],\n","                                        100 * epoch_logs[out_labels[-1]]),\n","                                 validation=('[D loss: %g, acc.: %.2f%%] [G total_loss: %g, img_loss: %g, ' +\n","                                             'd_loss: %g, d_acc.: %.2f%%]') %\n","                                            (epoch_logs['val_' + out_labels[0]],\n","                                             100 * epoch_logs['val_' + out_labels[1]],\n","                                             epoch_logs['val_' + out_labels[2]],\n","                                             epoch_logs['val_' + out_labels[2]] - epoch_logs['val_' + out_labels[9]],\n","                                             epoch_logs['val_' + out_labels[9]],\n","                                             100 * epoch_logs['val_' + out_labels[-1]]))\n","        progress_bar.close()\n","    callbacks.on_train_end()\n","    hist_data = {\n","        'train_start': time_history.train_time_start,\n","        'train_time': time_history.train_time,\n","        'epoch_time': time_history.times,\n","        'history': history.history,\n","        'choice': np.where(choice == 1)[0].tolist()\n","    }\n","    return hist_data\n","\n","\n","hist_data = run_epoch(100)\n","\n","gan.save('model46log_100.h5')\n","generator.save('model46log_100_gen.h5')\n","discriminator.save('model46log_100_dis.h5')\n","with open('model_hist_data46.0log.json', 'w') as f:\n","    json.dump(hist_data, f)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /content/drive/My Drive/thesis/config_16x15_seq/helper.py:54: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/thesis/config_16x15_seq/helper.py:56: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","Generator model summary:\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, 7)      0                                            \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, None, 6)      0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 512)          403968      lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 1)            0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 1)            0           input_3[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 514)          0           bidirectional_1[0][0]            \n","                                                                 lambda_3[0][0]                   \n","                                                                 lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 8192)         4218880     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 2, 64, 64)    0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 4, 64, 64)    76          reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 4, 64, 64)    16          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 4, 64, 64)    0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 4, 64, 64)    148         leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 4, 64, 64)    16          conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)       (None, 4, 64, 64)    0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 8, 32, 32)    520         leaky_re_lu_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 8, 32, 32)    32          conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 8, 32, 32)    584         leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 8, 32, 32)    32          conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 8, 32, 32)    584         leaky_re_lu_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 8, 32, 32)    32          conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 16, 16, 16)   2064        leaky_re_lu_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 16, 16, 16)   64          conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 16, 16, 16)   2320        leaky_re_lu_6[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 16, 16, 16)   64          conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 16, 16, 16)   2320        leaky_re_lu_7[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 16, 16, 16)   64          conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 8, 8)     8224        leaky_re_lu_8[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 8, 8)     128         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)       (None, 32, 8, 8)     0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_9[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 8, 8)     128         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_10[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 32, 8, 8)     128         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 64, 4, 4)     32832       leaky_re_lu_11[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 64, 4, 4)     256         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 64, 4, 4)     36928       leaky_re_lu_12[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 64, 4, 4)     256         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 64, 4, 4)     36928       leaky_re_lu_13[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 64, 4, 4)     256         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 32, 8, 8)     32800       leaky_re_lu_14[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 32, 8, 8)     128         conv2d_transpose_1[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 64, 8, 8)     0           leaky_re_lu_11[0][0]             \n","                                                                 leaky_re_lu_15[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 32, 8, 8)     18464       concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 32, 8, 8)     128         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_16[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 32, 8, 8)     128         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 16)   8208        leaky_re_lu_17[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_2[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_18 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 32, 16, 16)   0           leaky_re_lu_8[0][0]              \n","                                                                 leaky_re_lu_18[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 16)   4624        concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 16, 16, 16)   64          conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_19 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_19[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 16, 16, 16)   64          conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_20 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_3 (Conv2DTrans (None, 8, 32, 32)    2056        leaky_re_lu_20[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 32, 32)    32          conv2d_transpose_3[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_21 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 16, 32, 32)   0           leaky_re_lu_5[0][0]              \n","                                                                 leaky_re_lu_21[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 8, 32, 32)    1160        concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 8, 32, 32)    32          conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_22 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 8, 32, 32)    584         leaky_re_lu_22[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 8, 32, 32)    32          conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_23 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_4 (Conv2DTrans (None, 4, 64, 64)    516         leaky_re_lu_23[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 4, 64, 64)    16          conv2d_transpose_4[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_24 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 8, 64, 64)    0           leaky_re_lu_2[0][0]              \n","                                                                 leaky_re_lu_24[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 4, 64, 64)    292         concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 4, 64, 64)    16          conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_25 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 4, 64, 64)    148         leaky_re_lu_25[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 4, 64, 64)    16          conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_26 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 64)           32960       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 2, 64, 64)    10          leaky_re_lu_26[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_27 (LeakyReLU)      (None, 64)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 64)           32960       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 2, 64, 64)    0           reshape_1[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            65          leaky_re_lu_27[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_28 (LeakyReLU)      (None, 64)           0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 2, 64, 64)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 1)            65          leaky_re_lu_28[0][0]             \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 2, 64, 64)    0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 4096, 1)      0           lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","repeat_vector_2 (RepeatVector)  (None, 4096, 1)      0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 1, 64, 64)    0           lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 1, 64, 64)    0           repeat_vector_1[0][0]            \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 1, 64, 64)    0           lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 1, 64, 64)    0           repeat_vector_2[0][0]            \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 1, 64, 64)    0           lambda_7[0][0]                   \n","                                                                 reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 1, 64, 64)    0           lambda_8[0][0]                   \n","                                                                 reshape_3[0][0]                  \n","==================================================================================================\n","Total params: 4,913,544\n","Trainable params: 4,912,448\n","Non-trainable params: 1,096\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Discriminator model summary:\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            (None, 1, 64, 64)    0                                            \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 1, 64, 64)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 16, 32, 32)   272         input_4[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 16, 32, 32)   272         input_5[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 16, 32, 32)   64          conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 16, 32, 32)   64          conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_29 (LeakyReLU)      (None, 16, 32, 32)   0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","leaky_re_lu_35 (LeakyReLU)      (None, 16, 32, 32)   0           batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 32, 16, 16)   8224        leaky_re_lu_29[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 32, 16, 16)   8224        leaky_re_lu_35[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 32, 16, 16)   128         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 32, 16, 16)   128         conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_30 (LeakyReLU)      (None, 32, 16, 16)   0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","leaky_re_lu_36 (LeakyReLU)      (None, 32, 16, 16)   0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 64, 8, 8)     32832       leaky_re_lu_30[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 64, 8, 8)     32832       leaky_re_lu_36[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 64, 8, 8)     256         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 64, 8, 8)     256         conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_31 (LeakyReLU)      (None, 64, 8, 8)     0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","leaky_re_lu_37 (LeakyReLU)      (None, 64, 8, 8)     0           batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 64, 4, 4)     65600       leaky_re_lu_31[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 64, 4, 4)     65600       leaky_re_lu_37[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 64, 4, 4)     256         conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 64, 4, 4)     256         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_32 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","leaky_re_lu_38 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 64, 2, 2)     65600       leaky_re_lu_32[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 64, 2, 2)     65600       leaky_re_lu_38[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 64, 2, 2)     256         conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 64, 2, 2)     256         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_33 (LeakyReLU)      (None, 64, 2, 2)     0           batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","leaky_re_lu_39 (LeakyReLU)      (None, 64, 2, 2)     0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 64, 1, 1)     65600       leaky_re_lu_33[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 64, 1, 1)     65600       leaky_re_lu_39[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 64, 1, 1)     256         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 64, 1, 1)     256         conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_34 (LeakyReLU)      (None, 64, 1, 1)     0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","leaky_re_lu_40 (LeakyReLU)      (None, 64, 1, 1)     0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 64)           0           leaky_re_lu_34[0][0]             \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 64)           0           leaky_re_lu_40[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 128)          0           flatten_1[0][0]                  \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 1)            129         concatenate_6[0][0]              \n","==================================================================================================\n","Total params: 478,817\n","Trainable params: 477,601\n","Non-trainable params: 1,216\n","__________________________________________________________________________________________________\n","Generative adversarial network model summary:\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, 7)      0                                            \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, None, 6)      0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 512)          403968      lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 1)            0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 1)            0           input_3[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 514)          0           bidirectional_1[0][0]            \n","                                                                 lambda_3[0][0]                   \n","                                                                 lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 8192)         4218880     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 2, 64, 64)    0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 4, 64, 64)    76          reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 4, 64, 64)    16          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 4, 64, 64)    0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 4, 64, 64)    148         leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 4, 64, 64)    16          conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)       (None, 4, 64, 64)    0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 8, 32, 32)    520         leaky_re_lu_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 8, 32, 32)    32          conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 8, 32, 32)    584         leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 8, 32, 32)    32          conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 8, 32, 32)    584         leaky_re_lu_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 8, 32, 32)    32          conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 16, 16, 16)   2064        leaky_re_lu_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 16, 16, 16)   64          conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 16, 16, 16)   2320        leaky_re_lu_6[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 16, 16, 16)   64          conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 16, 16, 16)   2320        leaky_re_lu_7[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 16, 16, 16)   64          conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 8, 8)     8224        leaky_re_lu_8[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 8, 8)     128         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)       (None, 32, 8, 8)     0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_9[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 8, 8)     128         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_10[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 32, 8, 8)     128         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 64, 4, 4)     32832       leaky_re_lu_11[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 64, 4, 4)     256         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 64, 4, 4)     36928       leaky_re_lu_12[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 64, 4, 4)     256         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 64, 4, 4)     36928       leaky_re_lu_13[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 64, 4, 4)     256         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 32, 8, 8)     32800       leaky_re_lu_14[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 32, 8, 8)     128         conv2d_transpose_1[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 64, 8, 8)     0           leaky_re_lu_11[0][0]             \n","                                                                 leaky_re_lu_15[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 32, 8, 8)     18464       concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 32, 8, 8)     128         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_16[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 32, 8, 8)     128         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 16)   8208        leaky_re_lu_17[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_2[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_18 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 32, 16, 16)   0           leaky_re_lu_8[0][0]              \n","                                                                 leaky_re_lu_18[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 16)   4624        concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 16, 16, 16)   64          conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_19 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_19[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 16, 16, 16)   64          conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_20 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_3 (Conv2DTrans (None, 8, 32, 32)    2056        leaky_re_lu_20[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 32, 32)    32          conv2d_transpose_3[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_21 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 16, 32, 32)   0           leaky_re_lu_5[0][0]              \n","                                                                 leaky_re_lu_21[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 8, 32, 32)    1160        concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 8, 32, 32)    32          conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_22 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 8, 32, 32)    584         leaky_re_lu_22[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 8, 32, 32)    32          conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_23 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_4 (Conv2DTrans (None, 4, 64, 64)    516         leaky_re_lu_23[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 4, 64, 64)    16          conv2d_transpose_4[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_24 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 8, 64, 64)    0           leaky_re_lu_2[0][0]              \n","                                                                 leaky_re_lu_24[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 4, 64, 64)    292         concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 4, 64, 64)    16          conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_25 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 4, 64, 64)    148         leaky_re_lu_25[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 4, 64, 64)    16          conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_26 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 64)           32960       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 2, 64, 64)    10          leaky_re_lu_26[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_27 (LeakyReLU)      (None, 64)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 64)           32960       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 2, 64, 64)    0           reshape_1[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            65          leaky_re_lu_27[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_28 (LeakyReLU)      (None, 64)           0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 2, 64, 64)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 1)            65          leaky_re_lu_28[0][0]             \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 2, 64, 64)    0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 4096, 1)      0           lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","repeat_vector_2 (RepeatVector)  (None, 4096, 1)      0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 1, 64, 64)    0           lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 1, 64, 64)    0           repeat_vector_1[0][0]            \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 1, 64, 64)    0           lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 1, 64, 64)    0           repeat_vector_2[0][0]            \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 1, 64, 64)    0           lambda_7[0][0]                   \n","                                                                 reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 1, 64, 64)    0           lambda_8[0][0]                   \n","                                                                 reshape_3[0][0]                  \n","__________________________________________________________________________________________________\n","model_2 (Model)                 (None, 1)            478817      multiply_1[0][0]                 \n","                                                                 multiply_2[0][0]                 \n","==================================================================================================\n","Total params: 5,392,361\n","Trainable params: 4,912,448\n","Non-trainable params: 479,913\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","Epoch 1/100:   0%|          | 0/8000 [00:00<?, ?samples/s, augment=1, train_on_batch=[D loss: [0.695669 0.842798], acc.: [62.50% 28.12%]] [G total_loss: 8.04025, img_loss: 7.38172, d_loss: 0.658526, d_acc.: 62.50%]]/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","Epoch 1/100: 100%|██████████| 8000/8000 [03:35<00:00, 41.86samples/s, train=[D loss: 0.565002, acc.: 79.27%] [G total_loss: 9.90113, img_loss: 9.2278, d_loss: 0.673331, d_acc.: 62.69%], validation=[D loss: 0.698162, acc.: 49.60%] [G total_loss: 9.2765, img_loss: 8.65223, d_loss: 0.62427, d_acc.: 100.00%]]\n","Epoch 2/100: 100%|██████████| 8000/8000 [03:32<00:00, 41.77samples/s, train=[D loss: 0.417959, acc.: 96.22%] [G total_loss: 9.45518, img_loss: 8.76434, d_loss: 0.690842, d_acc.: 60.16%], validation=[D loss: 0.770975, acc.: 50.00%] [G total_loss: 9.11234, img_loss: 8.67432, d_loss: 0.438017, d_acc.: 100.00%]]\n","Epoch 3/100: 100%|██████████| 8000/8000 [03:29<00:00, 43.05samples/s, train=[D loss: 0.368824, acc.: 96.87%] [G total_loss: 9.59906, img_loss: 8.74905, d_loss: 0.850011, d_acc.: 42.96%], validation=[D loss: 0.79505, acc.: 49.98%] [G total_loss: 8.65448, img_loss: 8.25039, d_loss: 0.404089, d_acc.: 100.00%]]\n","Epoch 4/100: 100%|██████████| 8000/8000 [03:28<00:00, 41.88samples/s, train=[D loss: 0.36361, acc.: 96.62%] [G total_loss: 9.74909, img_loss: 8.72352, d_loss: 1.02556, d_acc.: 29.34%], validation=[D loss: 0.919598, acc.: 49.85%] [G total_loss: 9.45871, img_loss: 9.14592, d_loss: 0.312787, d_acc.: 100.00%]]\n","Epoch 5/100: 100%|██████████| 8000/8000 [03:29<00:00, 43.54samples/s, train=[D loss: 0.390155, acc.: 93.69%] [G total_loss: 9.57129, img_loss: 8.29254, d_loss: 1.27876, d_acc.: 16.89%], validation=[D loss: 0.905307, acc.: 49.80%] [G total_loss: 10.147, img_loss: 9.83561, d_loss: 0.311405, d_acc.: 100.00%]]\n","Epoch 6/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.51samples/s, train=[D loss: 0.436506, acc.: 87.78%] [G total_loss: 8.94416, img_loss: 7.66589, d_loss: 1.27827, d_acc.: 12.50%], validation=[D loss: 0.862174, acc.: 50.00%] [G total_loss: 9.47045, img_loss: 9.14158, d_loss: 0.32887, d_acc.: 100.00%]]\n","Epoch 7/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.04samples/s, train=[D loss: 0.414532, acc.: 89.68%] [G total_loss: 8.34761, img_loss: 7.34803, d_loss: 0.999582, d_acc.: 33.05%], validation=[D loss: 0.75798, acc.: 45.90%] [G total_loss: 7.68118, img_loss: 7.20286, d_loss: 0.478322, d_acc.: 100.00%]]\n","Epoch 8/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.12samples/s, train=[D loss: 0.406448, acc.: 88.94%] [G total_loss: 7.9294, img_loss: 7.04434, d_loss: 0.885065, d_acc.: 42.24%], validation=[D loss: 0.848083, acc.: 49.98%] [G total_loss: 6.72383, img_loss: 6.39595, d_loss: 0.327879, d_acc.: 100.00%]]\n","Epoch 9/100: 100%|██████████| 8000/8000 [03:29<00:00, 41.72samples/s, train=[D loss: 0.368061, acc.: 91.62%] [G total_loss: 7.29836, img_loss: 6.67303, d_loss: 0.625333, d_acc.: 67.00%], validation=[D loss: 1.22699, acc.: 50.00%] [G total_loss: 5.96138, img_loss: 5.80604, d_loss: 0.155349, d_acc.: 100.00%]]\n","Epoch 10/100: 100%|██████████| 8000/8000 [03:28<00:00, 42.60samples/s, train=[D loss: 0.333018, acc.: 92.95%] [G total_loss: 6.55908, img_loss: 6.08313, d_loss: 0.475943, d_acc.: 81.50%], validation=[D loss: 1.16297, acc.: 50.00%] [G total_loss: 5.81678, img_loss: 5.63375, d_loss: 0.183033, d_acc.: 100.00%]]\n","Epoch 11/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.47samples/s, train=[D loss: 0.295766, acc.: 94.50%] [G total_loss: 6.00542, img_loss: 5.63542, d_loss: 0.369999, d_acc.: 89.96%], validation=[D loss: 1.42564, acc.: 50.00%] [G total_loss: 5.54487, img_loss: 5.40983, d_loss: 0.135033, d_acc.: 100.00%]]\n","Epoch 12/100: 100%|██████████| 8000/8000 [03:29<00:00, 40.68samples/s, train=[D loss: 0.283234, acc.: 95.85%] [G total_loss: 5.61713, img_loss: 5.32175, d_loss: 0.295385, d_acc.: 93.47%], validation=[D loss: 1.78569, acc.: 50.00%] [G total_loss: 5.45377, img_loss: 5.35954, d_loss: 0.0942256, d_acc.: 100.00%]]\n","Epoch 13/100: 100%|██████████| 8000/8000 [03:28<00:00, 41.76samples/s, train=[D loss: 0.24169, acc.: 97.84%] [G total_loss: 5.38401, img_loss: 5.12364, d_loss: 0.260365, d_acc.: 95.49%], validation=[D loss: 1.59852, acc.: 50.00%] [G total_loss: 5.83947, img_loss: 5.72033, d_loss: 0.119142, d_acc.: 100.00%]]\n","Epoch 14/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.29samples/s, train=[D loss: 0.238459, acc.: 97.78%] [G total_loss: 5.17581, img_loss: 4.93717, d_loss: 0.238643, d_acc.: 95.53%], validation=[D loss: 1.15094, acc.: 50.00%] [G total_loss: 5.44521, img_loss: 5.26323, d_loss: 0.18198, d_acc.: 100.00%]]\n","Epoch 15/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.47samples/s, train=[D loss: 0.22709, acc.: 98.57%] [G total_loss: 5.01212, img_loss: 4.79714, d_loss: 0.214984, d_acc.: 96.80%], validation=[D loss: 1.17068, acc.: 50.00%] [G total_loss: 5.03773, img_loss: 4.85163, d_loss: 0.1861, d_acc.: 100.00%]]\n","Epoch 16/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.64samples/s, train=[D loss: 0.216465, acc.: 98.83%] [G total_loss: 4.84498, img_loss: 4.65877, d_loss: 0.186208, d_acc.: 97.52%], validation=[D loss: 1.53651, acc.: 50.00%] [G total_loss: 5.31307, img_loss: 5.19143, d_loss: 0.121642, d_acc.: 100.00%]]\n","Epoch 17/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.85samples/s, train=[D loss: 0.212879, acc.: 99.09%] [G total_loss: 4.78727, img_loss: 4.57505, d_loss: 0.212226, d_acc.: 95.99%], validation=[D loss: 1.57711, acc.: 50.00%] [G total_loss: 5.26592, img_loss: 5.161, d_loss: 0.104924, d_acc.: 100.00%]]\n","Epoch 18/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.74samples/s, train=[D loss: 0.217327, acc.: 98.71%] [G total_loss: 4.62106, img_loss: 4.44926, d_loss: 0.171804, d_acc.: 97.78%], validation=[D loss: 1.62614, acc.: 50.00%] [G total_loss: 5.36731, img_loss: 5.27726, d_loss: 0.0900505, d_acc.: 100.00%]]\n","Epoch 19/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.01samples/s, train=[D loss: 0.247131, acc.: 96.83%] [G total_loss: 4.53914, img_loss: 4.3624, d_loss: 0.176733, d_acc.: 96.80%], validation=[D loss: 1.38759, acc.: 50.00%] [G total_loss: 5.36159, img_loss: 5.23513, d_loss: 0.126459, d_acc.: 100.00%]]\n","Epoch 20/100: 100%|██████████| 8000/8000 [03:32<00:00, 42.18samples/s, train=[D loss: 0.201449, acc.: 99.32%] [G total_loss: 4.47418, img_loss: 4.30462, d_loss: 0.169562, d_acc.: 98.02%], validation=[D loss: 1.571, acc.: 50.00%] [G total_loss: 4.4513, img_loss: 4.31798, d_loss: 0.13332, d_acc.: 100.00%]]\n","Epoch 21/100: 100%|██████████| 8000/8000 [03:30<00:00, 43.34samples/s, train=[D loss: 0.249246, acc.: 97.08%] [G total_loss: 4.36061, img_loss: 4.22572, d_loss: 0.134892, d_acc.: 99.19%], validation=[D loss: 1.22328, acc.: 50.00%] [G total_loss: 5.22127, img_loss: 5.0397, d_loss: 0.181564, d_acc.: 100.00%]]\n","Epoch 22/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.55samples/s, train=[D loss: 0.199903, acc.: 99.46%] [G total_loss: 4.26608, img_loss: 4.14745, d_loss: 0.118634, d_acc.: 98.88%], validation=[D loss: 1.02552, acc.: 50.00%] [G total_loss: 6.10154, img_loss: 5.87055, d_loss: 0.230988, d_acc.: 100.00%]]\n","Epoch 23/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.64samples/s, train=[D loss: 0.202366, acc.: 99.28%] [G total_loss: 4.23762, img_loss: 4.10552, d_loss: 0.132106, d_acc.: 97.71%], validation=[D loss: 1.93262, acc.: 50.00%] [G total_loss: 4.86164, img_loss: 4.77284, d_loss: 0.088797, d_acc.: 100.00%]]\n","Epoch 24/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.25samples/s, train=[D loss: 0.263517, acc.: 96.69%] [G total_loss: 4.15581, img_loss: 4.01477, d_loss: 0.141044, d_acc.: 98.61%], validation=[D loss: 1.17791, acc.: 50.00%] [G total_loss: 4.57366, img_loss: 4.38034, d_loss: 0.19332, d_acc.: 100.00%]]\n","Epoch 25/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.34samples/s, train=[D loss: 0.235592, acc.: 97.63%] [G total_loss: 4.088, img_loss: 3.96988, d_loss: 0.118126, d_acc.: 98.66%], validation=[D loss: 1.0421, acc.: 50.25%] [G total_loss: 4.42598, img_loss: 4.18398, d_loss: 0.242007, d_acc.: 99.00%]]\n","Epoch 26/100: 100%|██████████| 8000/8000 [03:29<00:00, 40.89samples/s, train=[D loss: 0.209791, acc.: 99.12%] [G total_loss: 4.03115, img_loss: 3.92672, d_loss: 0.104428, d_acc.: 98.90%], validation=[D loss: 1.32307, acc.: 49.75%] [G total_loss: 5.17177, img_loss: 4.99928, d_loss: 0.17249, d_acc.: 100.00%]]\n","Epoch 27/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.04samples/s, train=[D loss: 0.205464, acc.: 99.14%] [G total_loss: 3.97951, img_loss: 3.8836, d_loss: 0.0959106, d_acc.: 99.26%], validation=[D loss: 1.04185, acc.: 50.00%] [G total_loss: 4.33184, img_loss: 4.11211, d_loss: 0.219733, d_acc.: 100.00%]]\n","Epoch 28/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.47samples/s, train=[D loss: 0.231782, acc.: 98.22%] [G total_loss: 3.95844, img_loss: 3.83195, d_loss: 0.126493, d_acc.: 98.22%], validation=[D loss: 0.728563, acc.: 54.47%] [G total_loss: 5.14953, img_loss: 3.85496, d_loss: 1.29457, d_acc.: 0.50%]]\n","Epoch 29/100: 100%|██████████| 8000/8000 [03:27<00:00, 41.62samples/s, train=[D loss: 0.281196, acc.: 96.81%] [G total_loss: 3.98492, img_loss: 3.81044, d_loss: 0.174484, d_acc.: 98.22%], validation=[D loss: 1.9962, acc.: 49.80%] [G total_loss: 7.91196, img_loss: 4.38508, d_loss: 3.52689, d_acc.: 10.95%]]\n","Epoch 30/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.96samples/s, train=[D loss: 0.236829, acc.: 98.86%] [G total_loss: 3.96942, img_loss: 3.77871, d_loss: 0.190706, d_acc.: 97.17%], validation=[D loss: 1.88015, acc.: 49.95%] [G total_loss: 6.00208, img_loss: 3.94251, d_loss: 2.05957, d_acc.: 0.40%]]\n","Epoch 31/100: 100%|██████████| 8000/8000 [03:29<00:00, 43.36samples/s, train=[D loss: 0.221622, acc.: 98.83%] [G total_loss: 3.85185, img_loss: 3.73164, d_loss: 0.120204, d_acc.: 99.35%], validation=[D loss: 1.01656, acc.: 46.52%] [G total_loss: 5.75375, img_loss: 4.80581, d_loss: 0.947939, d_acc.: 43.75%]]\n","Epoch 32/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.14samples/s, train=[D loss: 0.207056, acc.: 99.24%] [G total_loss: 3.92924, img_loss: 3.81106, d_loss: 0.118174, d_acc.: 99.09%], validation=[D loss: 0.889854, acc.: 46.85%] [G total_loss: 5.36727, img_loss: 4.67396, d_loss: 0.693301, d_acc.: 45.80%]]\n","Epoch 33/100: 100%|██████████| 8000/8000 [03:29<00:00, 43.03samples/s, train=[D loss: 0.28555, acc.: 94.82%] [G total_loss: 3.83173, img_loss: 3.6493, d_loss: 0.182424, d_acc.: 96.15%], validation=[D loss: 0.819263, acc.: 47.50%] [G total_loss: 4.17964, img_loss: 3.76906, d_loss: 0.410577, d_acc.: 91.85%]]\n","Epoch 34/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.51samples/s, train=[D loss: 0.222501, acc.: 98.34%] [G total_loss: 3.63173, img_loss: 3.57064, d_loss: 0.0610869, d_acc.: 99.54%], validation=[D loss: 0.896785, acc.: 48.60%] [G total_loss: 4.55919, img_loss: 4.23752, d_loss: 0.321671, d_acc.: 99.85%]]\n","Epoch 35/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.10samples/s, train=[D loss: 0.247853, acc.: 97.34%] [G total_loss: 3.72463, img_loss: 3.57333, d_loss: 0.151302, d_acc.: 97.94%], validation=[D loss: 0.720634, acc.: 49.42%] [G total_loss: 5.09461, img_loss: 4.16945, d_loss: 0.925161, d_acc.: 5.80%]]\n","Epoch 36/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.47samples/s, train=[D loss: 0.23262, acc.: 97.62%] [G total_loss: 3.72671, img_loss: 3.59413, d_loss: 0.132589, d_acc.: 98.16%], validation=[D loss: 0.80117, acc.: 51.28%] [G total_loss: 5.89255, img_loss: 4.516, d_loss: 1.37655, d_acc.: 0.45%]]\n","Epoch 37/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.36samples/s, train=[D loss: 0.212034, acc.: 98.64%] [G total_loss: 3.59729, img_loss: 3.49323, d_loss: 0.104052, d_acc.: 99.31%], validation=[D loss: 1.52167, acc.: 50.00%] [G total_loss: 6.57853, img_loss: 3.69613, d_loss: 2.8824, d_acc.: 0.00%]]\n","Epoch 38/100: 100%|██████████| 8000/8000 [03:28<00:00, 42.36samples/s, train=[D loss: 0.203223, acc.: 98.98%] [G total_loss: 3.53335, img_loss: 3.46925, d_loss: 0.0641059, d_acc.: 99.46%], validation=[D loss: 1.11796, acc.: 49.30%] [G total_loss: 5.73222, img_loss: 3.81672, d_loss: 1.91551, d_acc.: 16.25%]]\n","Epoch 39/100: 100%|██████████| 8000/8000 [03:28<00:00, 43.16samples/s, train=[D loss: 0.205472, acc.: 98.88%] [G total_loss: 3.53288, img_loss: 3.45428, d_loss: 0.0785994, d_acc.: 99.04%], validation=[D loss: 5.64068, acc.: 50.00%] [G total_loss: 3.77348, img_loss: 3.77161, d_loss: 0.00186427, d_acc.: 100.00%]]\n","Epoch 40/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.50samples/s, train=[D loss: 0.201969, acc.: 98.94%] [G total_loss: 3.54301, img_loss: 3.4494, d_loss: 0.0936113, d_acc.: 98.92%], validation=[D loss: 5.50513, acc.: 50.00%] [G total_loss: 3.57291, img_loss: 3.57041, d_loss: 0.00250009, d_acc.: 100.00%]]\n","Epoch 41/100: 100%|██████████| 8000/8000 [03:27<00:00, 40.55samples/s, train=[D loss: 0.204661, acc.: 98.90%] [G total_loss: 3.42729, img_loss: 3.37132, d_loss: 0.0559746, d_acc.: 99.96%], validation=[D loss: 5.96506, acc.: 50.00%] [G total_loss: 3.86863, img_loss: 3.86775, d_loss: 0.000882298, d_acc.: 100.00%]]\n","Epoch 42/100: 100%|██████████| 8000/8000 [03:29<00:00, 41.79samples/s, train=[D loss: 0.198667, acc.: 99.19%] [G total_loss: 3.4605, img_loss: 3.38201, d_loss: 0.0784829, d_acc.: 99.17%], validation=[D loss: 4.92792, acc.: 50.00%] [G total_loss: 3.79758, img_loss: 3.79566, d_loss: 0.00192129, d_acc.: 100.00%]]\n","Epoch 43/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.79samples/s, train=[D loss: 0.192264, acc.: 99.48%] [G total_loss: 3.45278, img_loss: 3.36429, d_loss: 0.0884907, d_acc.: 98.81%], validation=[D loss: 4.68682, acc.: 50.00%] [G total_loss: 4.30308, img_loss: 4.30068, d_loss: 0.00240244, d_acc.: 100.00%]]\n","Epoch 44/100: 100%|██████████| 8000/8000 [03:28<00:00, 41.40samples/s, train=[D loss: 0.190598, acc.: 99.43%] [G total_loss: 3.40634, img_loss: 3.33135, d_loss: 0.0749816, d_acc.: 99.72%], validation=[D loss: 6.63236, acc.: 50.00%] [G total_loss: 3.55255, img_loss: 3.55217, d_loss: 0.000379053, d_acc.: 100.00%]]\n","Epoch 45/100: 100%|██████████| 8000/8000 [03:31<00:00, 41.12samples/s, train=[D loss: 0.20726, acc.: 98.85%] [G total_loss: 3.38078, img_loss: 3.28382, d_loss: 0.0969625, d_acc.: 99.16%], validation=[D loss: 2.15475, acc.: 50.00%] [G total_loss: 3.80571, img_loss: 3.77373, d_loss: 0.0319878, d_acc.: 100.00%]]\n","Epoch 46/100: 100%|██████████| 8000/8000 [03:31<00:00, 42.62samples/s, train=[D loss: 0.201887, acc.: 99.06%] [G total_loss: 3.37651, img_loss: 3.28139, d_loss: 0.0951109, d_acc.: 98.86%], validation=[D loss: 1.27574, acc.: 49.10%] [G total_loss: 3.90944, img_loss: 3.73668, d_loss: 0.172759, d_acc.: 98.70%]]\n","Epoch 47/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.49samples/s, train=[D loss: 0.20019, acc.: 98.91%] [G total_loss: 3.3089, img_loss: 3.2285, d_loss: 0.0804037, d_acc.: 99.52%], validation=[D loss: 0.687744, acc.: 60.02%] [G total_loss: 4.44583, img_loss: 3.46003, d_loss: 0.985799, d_acc.: 20.60%]]\n","Epoch 48/100: 100%|██████████| 8000/8000 [03:28<00:00, 42.03samples/s, train=[D loss: 0.192463, acc.: 99.36%] [G total_loss: 3.28469, img_loss: 3.20252, d_loss: 0.0821714, d_acc.: 99.56%], validation=[D loss: 5.48354, acc.: 50.00%] [G total_loss: 4.30996, img_loss: 4.30902, d_loss: 0.000933558, d_acc.: 100.00%]]\n","Epoch 49/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.93samples/s, train=[D loss: 0.217566, acc.: 98.34%] [G total_loss: 3.31473, img_loss: 3.22529, d_loss: 0.089445, d_acc.: 99.19%], validation=[D loss: 3.16986, acc.: 50.00%] [G total_loss: 3.69282, img_loss: 3.68181, d_loss: 0.0110111, d_acc.: 100.00%]]\n","Epoch 50/100: 100%|██████████| 8000/8000 [03:29<00:00, 41.90samples/s, train=[D loss: 0.194283, acc.: 99.23%] [G total_loss: 3.27423, img_loss: 3.18611, d_loss: 0.0881221, d_acc.: 99.55%], validation=[D loss: 2.43481, acc.: 50.00%] [G total_loss: 3.90485, img_loss: 3.88528, d_loss: 0.0195682, d_acc.: 100.00%]]\n","Epoch 51/100: 100%|██████████| 8000/8000 [03:27<00:00, 41.81samples/s, train=[D loss: 0.185788, acc.: 99.58%] [G total_loss: 3.29381, img_loss: 3.18508, d_loss: 0.108731, d_acc.: 98.75%], validation=[D loss: 3.46691, acc.: 50.00%] [G total_loss: 3.70826, img_loss: 3.69976, d_loss: 0.00850064, d_acc.: 100.00%]]\n","Epoch 52/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.42samples/s, train=[D loss: 0.196679, acc.: 99.05%] [G total_loss: 3.28581, img_loss: 3.1884, d_loss: 0.0974038, d_acc.: 98.59%], validation=[D loss: 4.11696, acc.: 50.00%] [G total_loss: 3.5554, img_loss: 3.55088, d_loss: 0.00452204, d_acc.: 100.00%]]\n","Epoch 53/100: 100%|██████████| 8000/8000 [03:32<00:00, 38.48samples/s, train=[D loss: 0.193838, acc.: 99.40%] [G total_loss: 3.19869, img_loss: 3.11084, d_loss: 0.0878448, d_acc.: 99.25%], validation=[D loss: 3.08894, acc.: 50.00%] [G total_loss: 3.67854, img_loss: 3.66956, d_loss: 0.00897941, d_acc.: 100.00%]]\n","Epoch 54/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.04samples/s, train=[D loss: 0.194144, acc.: 99.16%] [G total_loss: 3.30333, img_loss: 3.19335, d_loss: 0.109977, d_acc.: 98.04%], validation=[D loss: 3.13137, acc.: 50.00%] [G total_loss: 3.60436, img_loss: 3.59317, d_loss: 0.0111955, d_acc.: 100.00%]]\n","Epoch 55/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.93samples/s, train=[D loss: 0.190526, acc.: 99.38%] [G total_loss: 3.19653, img_loss: 3.10139, d_loss: 0.0951448, d_acc.: 99.12%], validation=[D loss: 5.19967, acc.: 50.00%] [G total_loss: 3.56324, img_loss: 3.56181, d_loss: 0.00142549, d_acc.: 100.00%]]\n","Epoch 56/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.29samples/s, train=[D loss: 0.192384, acc.: 99.34%] [G total_loss: 3.1303, img_loss: 3.0493, d_loss: 0.0810017, d_acc.: 99.55%], validation=[D loss: 3.88191, acc.: 50.00%] [G total_loss: 3.323, img_loss: 3.31845, d_loss: 0.00454665, d_acc.: 100.00%]]\n","Epoch 57/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.05samples/s, train=[D loss: 0.19536, acc.: 99.13%] [G total_loss: 3.14843, img_loss: 3.06028, d_loss: 0.0881501, d_acc.: 99.21%], validation=[D loss: 3.57971, acc.: 50.00%] [G total_loss: 3.25676, img_loss: 3.25018, d_loss: 0.00657816, d_acc.: 100.00%]]\n","Epoch 58/100: 100%|██████████| 8000/8000 [03:27<00:00, 42.37samples/s, train=[D loss: 0.189305, acc.: 99.53%] [G total_loss: 3.13728, img_loss: 3.04067, d_loss: 0.0966108, d_acc.: 99.30%], validation=[D loss: 3.59863, acc.: 50.00%] [G total_loss: 3.33234, img_loss: 3.32868, d_loss: 0.00366257, d_acc.: 100.00%]]\n","Epoch 59/100: 100%|██████████| 8000/8000 [03:26<00:00, 43.46samples/s, train=[D loss: 0.195959, acc.: 99.22%] [G total_loss: 3.15112, img_loss: 3.04924, d_loss: 0.101882, d_acc.: 98.62%], validation=[D loss: 4.87601, acc.: 50.00%] [G total_loss: 3.31908, img_loss: 3.31761, d_loss: 0.00146986, d_acc.: 100.00%]]\n","Epoch 60/100: 100%|██████████| 8000/8000 [03:25<00:00, 42.38samples/s, train=[D loss: 0.19533, acc.: 99.11%] [G total_loss: 3.0801, img_loss: 2.99607, d_loss: 0.0840367, d_acc.: 99.95%], validation=[D loss: 6.88684, acc.: 50.00%] [G total_loss: 3.37088, img_loss: 3.37083, d_loss: 5.31213e-05, d_acc.: 100.00%]]\n","Epoch 61/100: 100%|██████████| 8000/8000 [03:25<00:00, 43.16samples/s, train=[D loss: 0.189327, acc.: 99.32%] [G total_loss: 3.10597, img_loss: 3.01206, d_loss: 0.0939027, d_acc.: 98.61%], validation=[D loss: 4.96191, acc.: 50.00%] [G total_loss: 3.97418, img_loss: 3.973, d_loss: 0.00118455, d_acc.: 100.00%]]\n","Epoch 62/100: 100%|██████████| 8000/8000 [03:27<00:00, 42.53samples/s, train=[D loss: 0.187539, acc.: 99.56%] [G total_loss: 3.05185, img_loss: 2.96242, d_loss: 0.0894261, d_acc.: 99.52%], validation=[D loss: 4.4005, acc.: 50.00%] [G total_loss: 3.60334, img_loss: 3.60008, d_loss: 0.00325307, d_acc.: 100.00%]]\n","Epoch 63/100: 100%|██████████| 8000/8000 [03:28<00:00, 41.57samples/s, train=[D loss: 0.190229, acc.: 99.46%] [G total_loss: 3.08241, img_loss: 2.97869, d_loss: 0.10372, d_acc.: 98.80%], validation=[D loss: 6.37762, acc.: 50.00%] [G total_loss: 3.37185, img_loss: 3.3714, d_loss: 0.000444745, d_acc.: 100.00%]]\n","Epoch 64/100: 100%|██████████| 8000/8000 [03:28<00:00, 42.43samples/s, train=[D loss: 0.195313, acc.: 99.12%] [G total_loss: 3.05063, img_loss: 2.96539, d_loss: 0.0852338, d_acc.: 99.31%], validation=[D loss: 6.12129, acc.: 50.00%] [G total_loss: 3.47212, img_loss: 3.47116, d_loss: 0.000961209, d_acc.: 100.00%]]\n","Epoch 65/100: 100%|██████████| 8000/8000 [03:32<00:00, 41.67samples/s, train=[D loss: 0.185453, acc.: 99.35%] [G total_loss: 3.00748, img_loss: 2.92937, d_loss: 0.0781175, d_acc.: 99.78%], validation=[D loss: 5.54181, acc.: 50.00%] [G total_loss: 3.39177, img_loss: 3.38988, d_loss: 0.0018884, d_acc.: 100.00%]]\n","Epoch 66/100: 100%|██████████| 8000/8000 [03:32<00:00, 41.94samples/s, train=[D loss: 0.186275, acc.: 99.51%] [G total_loss: 2.9944, img_loss: 2.91489, d_loss: 0.0795135, d_acc.: 99.45%], validation=[D loss: 4.56881, acc.: 50.00%] [G total_loss: 3.23522, img_loss: 3.23142, d_loss: 0.0038043, d_acc.: 100.00%]]\n","Epoch 67/100: 100%|██████████| 8000/8000 [03:37<00:00, 41.05samples/s, train=[D loss: 0.190268, acc.: 99.25%] [G total_loss: 2.98027, img_loss: 2.88976, d_loss: 0.0905063, d_acc.: 99.59%], validation=[D loss: 4.39798, acc.: 50.00%] [G total_loss: 3.2184, img_loss: 3.21416, d_loss: 0.00423571, d_acc.: 100.00%]]\n","Epoch 68/100: 100%|██████████| 8000/8000 [03:38<00:00, 41.24samples/s, train=[D loss: 0.190516, acc.: 99.29%] [G total_loss: 2.97631, img_loss: 2.88073, d_loss: 0.095575, d_acc.: 99.14%], validation=[D loss: 1.90876, acc.: 50.00%] [G total_loss: 3.34714, img_loss: 3.29397, d_loss: 0.0531713, d_acc.: 100.00%]]\n","Epoch 69/100: 100%|██████████| 8000/8000 [03:37<00:00, 41.12samples/s, train=[D loss: 0.191328, acc.: 99.27%] [G total_loss: 2.95549, img_loss: 2.86902, d_loss: 0.0864751, d_acc.: 99.10%], validation=[D loss: 4.65374, acc.: 50.00%] [G total_loss: 3.33047, img_loss: 3.32802, d_loss: 0.00244452, d_acc.: 100.00%]]\n","Epoch 70/100: 100%|██████████| 8000/8000 [03:33<00:00, 41.62samples/s, train=[D loss: 0.186315, acc.: 99.54%] [G total_loss: 2.92193, img_loss: 2.84206, d_loss: 0.0798697, d_acc.: 99.50%], validation=[D loss: 5.12015, acc.: 50.00%] [G total_loss: 3.49773, img_loss: 3.49598, d_loss: 0.00175251, d_acc.: 100.00%]]\n","Epoch 71/100: 100%|██████████| 8000/8000 [03:32<00:00, 41.04samples/s, train=[D loss: 0.192602, acc.: 99.37%] [G total_loss: 2.93209, img_loss: 2.83294, d_loss: 0.0991458, d_acc.: 98.71%], validation=[D loss: 2.78553, acc.: 50.00%] [G total_loss: 4.13321, img_loss: 4.11776, d_loss: 0.0154442, d_acc.: 100.00%]]\n","Epoch 72/100: 100%|██████████| 8000/8000 [03:33<00:00, 41.82samples/s, train=[D loss: 0.181215, acc.: 99.84%] [G total_loss: 2.89669, img_loss: 2.81714, d_loss: 0.0795481, d_acc.: 99.65%], validation=[D loss: 1.78491, acc.: 49.88%] [G total_loss: 3.34194, img_loss: 3.27666, d_loss: 0.0652788, d_acc.: 100.00%]]\n","Epoch 73/100: 100%|██████████| 8000/8000 [03:31<00:00, 42.11samples/s, train=[D loss: 0.189956, acc.: 99.37%] [G total_loss: 2.89356, img_loss: 2.8187, d_loss: 0.074858, d_acc.: 99.79%], validation=[D loss: 6.54282, acc.: 50.00%] [G total_loss: 3.32611, img_loss: 3.32592, d_loss: 0.000191886, d_acc.: 100.00%]]\n","Epoch 74/100: 100%|██████████| 8000/8000 [03:34<00:00, 41.37samples/s, train=[D loss: 0.182781, acc.: 99.78%] [G total_loss: 2.8541, img_loss: 2.78565, d_loss: 0.0684528, d_acc.: 100.00%], validation=[D loss: 6.65858, acc.: 50.00%] [G total_loss: 3.27423, img_loss: 3.27409, d_loss: 0.000142984, d_acc.: 100.00%]]\n","Epoch 75/100: 100%|██████████| 8000/8000 [03:31<00:00, 39.67samples/s, train=[D loss: 0.191858, acc.: 99.24%] [G total_loss: 2.91443, img_loss: 2.83531, d_loss: 0.0791292, d_acc.: 99.60%], validation=[D loss: 7.2143, acc.: 50.00%] [G total_loss: 3.05369, img_loss: 3.05367, d_loss: 2.51338e-05, d_acc.: 100.00%]]\n","Epoch 76/100: 100%|██████████| 8000/8000 [03:31<00:00, 42.29samples/s, train=[D loss: 0.18518, acc.: 99.66%] [G total_loss: 2.88701, img_loss: 2.80309, d_loss: 0.0839226, d_acc.: 99.31%], validation=[D loss: 6.0115, acc.: 50.00%] [G total_loss: 3.2891, img_loss: 3.28868, d_loss: 0.000413138, d_acc.: 100.00%]]\n","Epoch 77/100: 100%|██████████| 8000/8000 [03:31<00:00, 41.87samples/s, train=[D loss: 0.187849, acc.: 99.48%] [G total_loss: 2.84481, img_loss: 2.75739, d_loss: 0.0874185, d_acc.: 99.59%], validation=[D loss: 6.25179, acc.: 50.00%] [G total_loss: 3.17272, img_loss: 3.17224, d_loss: 0.000475361, d_acc.: 100.00%]]\n","Epoch 78/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.55samples/s, train=[D loss: 0.186092, acc.: 99.66%] [G total_loss: 2.82883, img_loss: 2.74354, d_loss: 0.0852909, d_acc.: 99.49%], validation=[D loss: 6.0262, acc.: 50.00%] [G total_loss: 3.76619, img_loss: 3.76556, d_loss: 0.000624748, d_acc.: 100.00%]]\n","Epoch 79/100: 100%|██████████| 8000/8000 [03:29<00:00, 41.88samples/s, train=[D loss: 0.185493, acc.: 99.64%] [G total_loss: 2.82812, img_loss: 2.73765, d_loss: 0.0904683, d_acc.: 99.49%], validation=[D loss: 5.47034, acc.: 50.00%] [G total_loss: 3.23759, img_loss: 3.23447, d_loss: 0.00311844, d_acc.: 100.00%]]\n","Epoch 80/100: 100%|██████████| 8000/8000 [03:31<00:00, 41.40samples/s, train=[D loss: 0.190256, acc.: 99.39%] [G total_loss: 2.80823, img_loss: 2.71532, d_loss: 0.0929144, d_acc.: 99.30%], validation=[D loss: 7.32719, acc.: 50.00%] [G total_loss: 3.16966, img_loss: 3.16964, d_loss: 1.85845e-05, d_acc.: 100.00%]]\n","Epoch 81/100: 100%|██████████| 8000/8000 [03:32<00:00, 40.98samples/s, train=[D loss: 0.183537, acc.: 99.80%] [G total_loss: 2.78154, img_loss: 2.70352, d_loss: 0.0780207, d_acc.: 99.86%], validation=[D loss: 6.79309, acc.: 50.00%] [G total_loss: 3.24356, img_loss: 3.24344, d_loss: 0.000127685, d_acc.: 100.00%]]\n","Epoch 82/100: 100%|██████████| 8000/8000 [03:33<00:00, 40.86samples/s, train=[D loss: 0.181762, acc.: 99.94%] [G total_loss: 2.7837, img_loss: 2.6865, d_loss: 0.0972062, d_acc.: 99.16%], validation=[D loss: 3.59758, acc.: 50.00%] [G total_loss: 3.20278, img_loss: 3.19119, d_loss: 0.0115875, d_acc.: 100.00%]]\n","Epoch 83/100: 100%|██████████| 8000/8000 [03:35<00:00, 41.08samples/s, train=[D loss: 0.188375, acc.: 99.64%] [G total_loss: 2.76889, img_loss: 2.68786, d_loss: 0.0810293, d_acc.: 99.59%], validation=[D loss: 5.80934, acc.: 50.00%] [G total_loss: 3.09737, img_loss: 3.09591, d_loss: 0.00145527, d_acc.: 100.00%]]\n","Epoch 84/100: 100%|██████████| 8000/8000 [03:31<00:00, 42.48samples/s, train=[D loss: 0.183207, acc.: 99.64%] [G total_loss: 2.76315, img_loss: 2.67936, d_loss: 0.0837919, d_acc.: 99.12%], validation=[D loss: 5.46512, acc.: 50.00%] [G total_loss: 3.44918, img_loss: 3.44778, d_loss: 0.0013971, d_acc.: 100.00%]]\n","Epoch 85/100: 100%|██████████| 8000/8000 [03:29<00:00, 42.83samples/s, train=[D loss: 0.185674, acc.: 99.51%] [G total_loss: 2.75449, img_loss: 2.66015, d_loss: 0.0943355, d_acc.: 99.06%], validation=[D loss: 6.57004, acc.: 50.00%] [G total_loss: 3.2534, img_loss: 3.2531, d_loss: 0.000302967, d_acc.: 100.00%]]\n","Epoch 86/100: 100%|██████████| 8000/8000 [03:31<00:00, 41.05samples/s, train=[D loss: 0.180568, acc.: 99.94%] [G total_loss: 2.72739, img_loss: 2.63015, d_loss: 0.0972386, d_acc.: 99.20%], validation=[D loss: 5.62912, acc.: 50.00%] [G total_loss: 3.28234, img_loss: 3.28059, d_loss: 0.00174671, d_acc.: 100.00%]]\n","Epoch 87/100: 100%|██████████| 8000/8000 [03:30<00:00, 41.89samples/s, train=[D loss: 0.180674, acc.: 99.91%] [G total_loss: 2.67567, img_loss: 2.60577, d_loss: 0.0698933, d_acc.: 99.95%], validation=[D loss: 1.66139, acc.: 50.00%] [G total_loss: 3.36537, img_loss: 3.23582, d_loss: 0.129554, d_acc.: 99.70%]]\n","Epoch 88/100: 100%|██████████| 8000/8000 [03:35<00:00, 40.72samples/s, train=[D loss: 0.190876, acc.: 99.32%] [G total_loss: 2.74933, img_loss: 2.64731, d_loss: 0.102019, d_acc.: 98.98%], validation=[D loss: 2.68414, acc.: 50.00%] [G total_loss: 3.25968, img_loss: 3.23927, d_loss: 0.0204066, d_acc.: 100.00%]]\n","Epoch 89/100: 100%|██████████| 8000/8000 [03:39<00:00, 40.60samples/s, train=[D loss: 0.181809, acc.: 99.70%] [G total_loss: 2.69768, img_loss: 2.6182, d_loss: 0.0794847, d_acc.: 99.58%], validation=[D loss: 1.61297, acc.: 49.93%] [G total_loss: 3.23604, img_loss: 3.13142, d_loss: 0.10462, d_acc.: 99.50%]]\n","Epoch 90/100: 100%|██████████| 8000/8000 [03:37<00:00, 40.09samples/s, train=[D loss: 0.183077, acc.: 99.69%] [G total_loss: 2.68479, img_loss: 2.61177, d_loss: 0.0730194, d_acc.: 99.45%], validation=[D loss: 1.54287, acc.: 54.65%] [G total_loss: 6.61929, img_loss: 3.19281, d_loss: 3.42648, d_acc.: 10.20%]]\n","Epoch 91/100: 100%|██████████| 8000/8000 [03:36<00:00, 41.00samples/s, train=[D loss: 0.181323, acc.: 99.70%] [G total_loss: 2.66093, img_loss: 2.59412, d_loss: 0.0668024, d_acc.: 99.60%], validation=[D loss: 3.1503, acc.: 50.08%] [G total_loss: 10.0083, img_loss: 3.12971, d_loss: 6.87859, d_acc.: 0.00%]]\n","Epoch 92/100: 100%|██████████| 8000/8000 [03:34<00:00, 40.56samples/s, train=[D loss: 0.18449, acc.: 99.64%] [G total_loss: 2.65384, img_loss: 2.58369, d_loss: 0.07015, d_acc.: 99.75%], validation=[D loss: 6.20287, acc.: 50.00%] [G total_loss: 15.8039, img_loss: 3.0917, d_loss: 12.7122, d_acc.: 0.00%]]\n","Epoch 93/100: 100%|██████████| 8000/8000 [03:36<00:00, 40.59samples/s, train=[D loss: 0.182906, acc.: 99.88%] [G total_loss: 2.63155, img_loss: 2.55272, d_loss: 0.0788326, d_acc.: 99.58%], validation=[D loss: 1.90554, acc.: 50.62%] [G total_loss: 7.26762, img_loss: 3.09345, d_loss: 4.17417, d_acc.: 0.35%]]\n","Epoch 94/100: 100%|██████████| 8000/8000 [03:32<00:00, 41.69samples/s, train=[D loss: 0.182452, acc.: 99.71%] [G total_loss: 2.61632, img_loss: 2.54506, d_loss: 0.0712584, d_acc.: 99.54%], validation=[D loss: 1.13337, acc.: 49.88%] [G total_loss: 4.43323, img_loss: 3.25511, d_loss: 1.17812, d_acc.: 36.15%]]\n","Epoch 95/100: 100%|██████████| 8000/8000 [03:30<00:00, 42.60samples/s, train=[D loss: 0.185176, acc.: 99.65%] [G total_loss: 2.67466, img_loss: 2.57896, d_loss: 0.0956942, d_acc.: 99.06%], validation=[D loss: 1.02412, acc.: 50.60%] [G total_loss: 3.63597, img_loss: 3.08555, d_loss: 0.550416, d_acc.: 77.70%]]\n","Epoch 96/100: 100%|██████████| 8000/8000 [03:31<00:00, 41.41samples/s, train=[D loss: 0.18448, acc.: 99.67%] [G total_loss: 2.63665, img_loss: 2.55082, d_loss: 0.0858394, d_acc.: 99.12%], validation=[D loss: 2.90851, acc.: 49.95%] [G total_loss: 3.45973, img_loss: 3.44149, d_loss: 0.0182461, d_acc.: 99.95%]]\n","Epoch 97/100: 100%|██████████| 8000/8000 [03:34<00:00, 41.58samples/s, train=[D loss: 0.188986, acc.: 99.57%] [G total_loss: 2.66179, img_loss: 2.56026, d_loss: 0.101533, d_acc.: 99.16%], validation=[D loss: 2.32094, acc.: 49.80%] [G total_loss: 3.31098, img_loss: 3.13876, d_loss: 0.17222, d_acc.: 91.20%]]\n","Epoch 98/100: 100%|██████████| 8000/8000 [03:31<00:00, 42.35samples/s, train=[D loss: 0.18401, acc.: 99.83%] [G total_loss: 2.68136, img_loss: 2.55686, d_loss: 0.124499, d_acc.: 98.15%], validation=[D loss: 2.51932, acc.: 50.82%] [G total_loss: 3.46391, img_loss: 3.31187, d_loss: 0.152045, d_acc.: 95.40%]]\n","Epoch 99/100: 100%|██████████| 8000/8000 [03:32<00:00, 41.83samples/s, train=[D loss: 0.184022, acc.: 99.63%] [G total_loss: 2.61327, img_loss: 2.52986, d_loss: 0.0834086, d_acc.: 99.60%], validation=[D loss: 3.13048, acc.: 50.00%] [G total_loss: 3.07918, img_loss: 3.06286, d_loss: 0.0163192, d_acc.: 100.00%]]\n","Epoch 100/100: 100%|██████████| 8000/8000 [03:31<00:00, 42.04samples/s, train=[D loss: 0.178141, acc.: 99.86%] [G total_loss: 2.61244, img_loss: 2.50075, d_loss: 0.111695, d_acc.: 98.95%], validation=[D loss: 2.54594, acc.: 50.00%] [G total_loss: 3.25711, img_loss: 3.21853, d_loss: 0.0385798, d_acc.: 100.00%]]\n","/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:883: UserWarning: Layer bidirectional_1 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'lambda_5/Squeeze:0' shape=(?, ?) dtype=float32>}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n","  '. They will not be included '\n"],"name":"stderr"}]}]}