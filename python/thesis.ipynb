{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"thesis.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNuDgFb7Sj7Il/Qaotd0RZz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EchhwRUwN7RD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"4f3ec57f-327e-4959-e716-08c4174b4c09","executionInfo":{"status":"ok","timestamp":1578880411776,"user_tz":-480,"elapsed":20812,"user":{"displayName":"Diannata Rahman Yuliansyah","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mACNmrgOLXcejtf_jjwi06X_WG54VXOBtt4my1m=s64","userId":"04644630010819509714"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2P4yvHx1XEFz","colab_type":"code","outputId":"b5fa6a57-2ea4-419c-c357-57b0ada137fa","executionInfo":{"status":"ok","timestamp":1578881505235,"user_tz":-480,"elapsed":647,"user":{"displayName":"Diannata Rahman Yuliansyah","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mACNmrgOLXcejtf_jjwi06X_WG54VXOBtt4my1m=s64","userId":"04644630010819509714"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd '/content/drive/My Drive/thesis'\n","import config_16x15_seq\n","%cd '/content/drive/My Drive/thesis/config_16x15_seq'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/thesis\n","/content/drive/My Drive/thesis/config_16x15_seq\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rs_DcUTNXNNg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":97},"outputId":"cebc187e-9cb7-4a9e-cc11-784e08b79782","executionInfo":{"status":"ok","timestamp":1578880446245,"user_tz":-480,"elapsed":2054,"user":{"displayName":"Diannata Rahman Yuliansyah","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mACNmrgOLXcejtf_jjwi06X_WG54VXOBtt4my1m=s64","userId":"04644630010819509714"}}},"source":["import os\n","import pprint\n","import tensorflow as tf\n","\n","if 'COLAB_TPU_ADDR' not in os.environ:\n","    device_name = tf.test.gpu_device_name()\n","    if device_name != '/device:GPU:0':\n","        print('ERROR!')\n","    else:\n","        print('Found GPU at: {}'.format(device_name))\n","else:\n","    tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n","    print('TPU address is', tpu_address)\n","    with tf.Session(tpu_address) as session:\n","        devices = session.list_devices()\n","        print('TPU devices:')\n","        pprint.pprint(devices)\n","\n"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["ERROR!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wqojsneLXlfE","colab_type":"code","colab":{}},"source":["from config_16x15_seq.builder import *\n","from tqdm import tqdm\n","from keras.callbacks import BaseLogger, History, CallbackList\n","from scipy.ndimage import gaussian_filter\n","import copy\n","\n","MU_training = MU['training']\n","MU_norm_training = MU_norm['training']\n","PHI_meas_training = PHI_meas['training']\n","MUa_training = MUa['training']\n","MUsp_training = MUsp['training']\n","freq_training = freq['training']\n","d_training = d['training']\n","lr = 0.0002\n","beta_1 = 0.5\n","# clip_value = 0.01\n","optimizer = Adam(lr=lr, beta_1=beta_1)\n","# optimizer = RMSprop(lr=lr)\n","generator = primary_net()\n","print('Generator model summary:')\n","generator.summary()\n","discriminator = secondary_net()\n","discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[custom_binary_accuracy])\n","print('Discriminator model summary:')\n","discriminator.summary()\n","discriminator.trainable = False\n","# inputs = Input((PHI_meas_training.shape[1],))\n","outputs = generator.outputs\n","outputs.append(discriminator(outputs[:2]))\n","gan = Model(inputs=generator.inputs, outputs=outputs)\n","loss = ['mse', 'mse', 'mse', 'mse', 'mse', 'mse', 'binary_crossentropy']\n","loss_weights = [0, 0, 5, 5, 1e4, 1, 1]\n","metrics = ['mae', 'mse', 'mape', 'msle', 'logcosh', 'cosine', custom_binary_accuracy]\n","gan.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer, metrics=metrics)\n","print('Generative adversarial network model summary:')\n","gan.summary()\n","history = History()\n","logger = BaseLogger(stateful_metrics=['discriminator_' + s for s in discriminator.stateful_metric_names] +\n","                                     ['gan_' + s for s in gan.stateful_metric_names])\n","# checkpoint = callbacks.ModelCheckpoint('model_test{epoch:02d}.h5', verbose=1)\n","time_history = TimeHistory()\n","callbacks = [logger, time_history, history]\n","out_labels = ['discriminator_' + s for s in discriminator.metrics_names] + ['gan_' + s for s in gan.metrics_names]\n","callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n","callbacks = CallbackList(callbacks)\n","# callbacks = [TimeHistory(), EarlyStopping(monitor='loss', min_delta=0.1, patience=100,\n","#                                           restore_best_weights=True, verbose=1)]\n","# early_stop = EarlyStopping(patience=200, verbose=1)\n","choice = np.zeros(data_size, dtype='uint8')\n","choice[np.random.choice(data_size, val_size, replace=False)] = 1\n","MU_training_train = MU_training[choice == 0]\n","MU_training_val = MU_training[choice == 1]\n","MU_norm_training_train = MU_norm_training[choice == 0]\n","MU_norm_training_val = MU_norm_training[choice == 1]\n","PHI_meas_training_train = PHI_meas_training[choice == 0]\n","PHI_meas_training_val = PHI_meas_training[choice == 1]\n","freq_training_train = freq_training[choice == 0]\n","freq_training_val = freq_training[choice == 1]\n","d_training_train = d_training[choice == 0]\n","d_training_val = d_training[choice == 1]\n","MUa_train = MUa_training[choice == 0]\n","MUa_val = MUa_training[choice == 1]\n","MUsp_train = MUsp_training[choice == 0]\n","MUsp_val = MUsp_training[choice == 1]\n","# choice = data_cat\n","batch_size = 32\n","prob_param = 20.0\n","prob_param2 = 1.0\n","# label_softness = 0.1\n","# metrics_eval_n = 50\n","# metrics_eval_step = 20\n","hard_valid = np.ones(val_size)\n","hard_fake = np.zeros(val_size)\n","\n","\n","# soft_label = False\n","# augment = True\n","# min_img_loss_avg = np.inf\n","# min_dis_loss_avg = np.inf\n","# img_loss_baseline_factor = 1.1\n","# img_loss_threshold = 1.2\n","# img_loss_threshold2 = 0.6\n","# prob_augment = 0.5\n","# gan_d_acc = [0.5, 0.5]\n","# init_train = 100\n","\n","N_count_train = np.zeros_like(N_count)\n","n = 0\n","for i in range(len(N_count)):\n","    ni = N_count[i]\n","    N_count_train[i] = np.where(choice[n:(n + ni)] == 0)[0].size\n","    n += ni\n","index_array = np.arange(train_size)\n","index_array2 = np.arange(train_size)\n","ia = np.arange(train_size)\n","n = N_count_train[0]\n","np.random.shuffle(index_array[:n])\n","np.random.shuffle(index_array2[:n])\n","for i in range(1, len(N_count_train) - 1):\n","    ni = N_count_train[i]\n","    np.random.shuffle(index_array[n:(n + ni)])\n","    np.random.shuffle(index_array2[n:(n + ni)])\n","    n += ni\n","np.random.shuffle(index_array[n:])\n","np.random.shuffle(index_array2[n:])\n","\n","\n","def run_epoch(epochs):\n","    callbacks.set_params({\n","        'batch_size': batch_size,\n","        'epochs': epochs,\n","        'steps': None,\n","        'samples': train_size,\n","        'verbose': 2,\n","        'do_validation': True,\n","        'metrics': callback_metrics,\n","    })\n","    callbacks.on_train_begin()\n","    for epoch in range(epochs):\n","        for m in discriminator.stateful_metric_functions:\n","            m.reset_states()\n","        for m in gan.stateful_metric_functions:\n","            m.reset_states()\n","        callbacks.on_epoch_begin(epoch)\n","        epoch_logs = {}\n","        progress_bar = None\n","        np.random.shuffle(ia)\n","        # n = N_count_train[0]\n","        # np.random.shuffle(ia[:n])\n","        # for i in range(1, len(N_count_train) - 1):\n","        #     ni = N_count_train[i]\n","        #     np.random.shuffle(ia[n:(n + ni)])\n","        #     n += ni\n","        # np.random.shuffle(ia[n:])\n","        num_batches = (train_size + batch_size - 1) // batch_size\n","        # if epoch > 0 and epoch % metrics_eval_step == 0:\n","        # hist_avg = {}\n","        # for k, v in history.history.items():\n","        #     if k.startswith('val_'):\n","        #         l = k[4:]\n","        #     else:\n","        #         l = k\n","        #     if l in logger.stateful_metrics:\n","        #         hist_avg[k] = v[-1]\n","        #     else:\n","        #         hist_avg[k] = np.average(v[-metrics_eval_n:])\n","        # img_loss_avg = hist_avg['val_gan_' + gan.metrics_names[0]] - hist_avg['val_gan_' + gan.metrics_names[7]]\n","        # dis_loss_avg = hist_avg['val_discriminator_' + discriminator.metrics_names[0]]\n","        # min_img_loss = np.min(history.history['val_gan_' + gan.metrics_names[0]]) \\\n","        #                - np.min(history.history['val_gan_' + gan.metrics_names[7]])\n","        # if img_loss_avg > min_img_loss_avg * img_loss_baseline_factor:\n","        #     if not augment:\n","        #         augment = True\n","        #     elif img_loss_avg >= img_loss_threshold and min_img_loss >= img_loss_threshold2:\n","        #         soft_label = True\n","        # elif dis_loss_avg > min_dis_loss_avg:\n","        #     if augment:\n","        #         augment = False\n","        #     elif img_loss_avg < img_loss_threshold:\n","        #         soft_label = False\n","        # img_loss_avg = np.average(history.history['val_gan_' + gan.metrics_names[0]][-metrics_eval_n:])\\\n","        #                - np.average(history.history['val_gan_' + gan.metrics_names[7]][-metrics_eval_n:])\n","        # if soft_label and img_loss_avg < img_loss_threshold:\n","        #     soft_label = False\n","        # if np.random.random() > 0.5:\n","        #     augment = not augment\n","        # min_img_loss_avg = min(img_loss_avg, min_img_loss_avg)\n","        # min_dis_loss_avg = min(dis_loss_avg, min_dis_loss_avg)\n","        # if epoch >= init_train:\n","        #     if gan_d_acc[0] < 0.5 or gan_d_acc[1] < 0.5:\n","        #         prob_augment = 0.8\n","        #     else:\n","        #         prob_augment = 0.5\n","        #     if augment == (np.random.random() < prob_augment):\n","        #         augment = not augment\n","        batch_end = 0\n","        # prob_augment = np.random.random()\n","        for batch_index in range(num_batches):\n","            augment = np.random.random() < 0.5\n","            batch_start = batch_end\n","            batch_end = min(train_size, batch_end + batch_size)\n","            batch_ids = index_array[ia[batch_start:batch_end]]\n","            batch_logs = {'batch': batch_index, 'size': len(batch_ids)}\n","            callbacks.on_batch_begin(batch_index, batch_logs)\n","            MU_gt = [MU_training_train[batch_ids, 0], MU_training_train[batch_ids, 1]]\n","            MU_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_gt]\n","            in_gen = [PHI_meas_training_train[batch_ids], freq_training_train[batch_ids], d_training_train[batch_ids]]\n","            if augment:\n","                noise_param = np.random.random(len(batch_ids))\n","                blur_param = np.random.random(len(batch_ids))\n","                noise_param *= blur_param\n","                MU_gt_aug = [np.copy(im) for im in MU_gt]\n","                for i, bi in enumerate(batch_ids):\n","                    temp = mask_image(MU_gt_aug[0][i, 0], MUa_train[bi])\n","                    temp = temp + temp * np.random.normal(scale=noise_param[i] * 0.5, size=temp.shape)\n","                    temp = gaussian_filter(temp, blur_param[i] * 5.0)\n","                    MU_gt_aug[0][i, 0] = mask_image(temp, 0.0)\n","                    temp = mask_image(MU_gt_aug[1][i, 0], MUsp_train[bi])\n","                    temp = temp + temp * np.random.normal(scale=noise_param[i] * 0.5, size=temp.shape)\n","                    temp = gaussian_filter(temp, blur_param[i] * 5.0)\n","                    MU_gt_aug[1][i, 0] = mask_image(temp, 0.0)\n","                # temp = np.clip(blur_param, prob_param2 / prob_param, 1.0 - prob_param2 / prob_param)\n","                valid = 1.0 - 0.1 * np.random.random(len(batch_ids))\n","                # valid = 1.0 - label_softness * np.random.beta(temp * prob_param, (1.0 - temp) * prob_param)\n","                # if soft_label:\n","                #     fake = label_softness * np.random.random(len(batch_ids))\n","                #     valid = 1.0 - label_softness * np.random.beta(temp * prob_param, (1.0 - temp) * prob_param)\n","                # else:\n","                #     fake = hard_fake[:len(batch_ids)]\n","                #     valid = hard_valid[:len(batch_ids)] - label_softness\n","            else:\n","                MU_gt_aug = MU_gt\n","                # valid = hard_valid[:len(batch_ids)]\n","                valid = 1.0 - 0.05 * np.random.random(len(batch_ids))\n","                # valid = 1.0 - label_softness * np.random.random(len(batch_ids))\n","                # valid = 1.0 - label_softness * np.random.beta(prob_param2,\n","                #                                               prob_param - prob_param2, size=len(batch_ids))\n","                # if soft_label:\n","                #     fake = label_softness * np.random.random(len(batch_ids))\n","                #     valid = 1.0 - label_softness * np.random.beta(\n","                #         prob_param2, prob_param - prob_param2, size=len(batch_ids))\n","                # else:\n","                #     fake = hard_fake[:len(batch_ids)]\n","                #     valid = hard_valid[:len(batch_ids)] - label_softness\n","            # fake = hard_fake[:len(batch_ids)]\n","            fake = 0.05 * np.random.random(len(batch_ids))\n","            # if epoch < init_train:\n","            #     valid -= label_softness * np.random.random(len(batch_ids)) * (init_train - epoch) / init_train\n","            #     fake += label_softness * np.random.random(len(batch_ids)) * (init_train - epoch) / init_train\n","            d_loss1 = discriminator.train_on_batch(MU_gt_aug, valid)\n","            # d_loss1 = discriminator.train_on_batch([np.concatenate([im1, im2], axis=0)\n","            #                                        for im1, im2 in zip(MU_gt_aug, MU_gt)],\n","            #                                        np.concatenate([valid, hard_valid[:len(batch_ids)]], axis=0),\n","            #                                        sample_weight=np.ones(2 * len(batch_ids)) * 0.5)\n","            d_loss2 = discriminator.train_on_batch(generator.predict(in_gen)[:2], fake)\n","            # d_loss = discriminator.train_on_batch([np.concatenate([im1, im2], axis=0)\n","            #                                        for im1, im2 in zip(MU_gt_aug, generator.predict(in_gen)[:2])],\n","            #                                       np.concatenate([valid, fake], axis=0))\n","            d_loss = np.zeros(len(discriminator.metrics_names))\n","            for i, l in enumerate(discriminator.metrics_names):\n","                if l in discriminator.stateful_metric_names:\n","                    d_loss[i] = d_loss2[i]\n","                else:\n","                    d_loss[i] = 0.5 * (d_loss1[i] + d_loss2[i])\n","            # for l in discriminator.layers:\n","            #     weights = l.get_weights()\n","            #     weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n","            #     l.set_weights(weights)\n","            batch_ids = index_array2[ia[batch_start:batch_end]]\n","            MU_gt = [MU_training_train[batch_ids, 0], MU_training_train[batch_ids, 1]]\n","            MU_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_gt]\n","            in_gen = [PHI_meas_training_train[batch_ids], freq_training_train[batch_ids], d_training_train[batch_ids]]\n","            MU_norm_gt = [MU_norm_training_train[batch_ids, 0], MU_norm_training_train[batch_ids, 1]]\n","            MU_norm_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_norm_gt]\n","            g_loss = gan.train_on_batch(in_gen, [MU_gt[0], MU_gt[1], MU_norm_gt[0], MU_norm_gt[1],\n","                                                 MUa_train[batch_ids], MUsp_train[batch_ids],\n","                                                 hard_valid[:len(batch_ids)]])\n","            for l, o in zip(out_labels, np.concatenate([d_loss, g_loss])):\n","                batch_logs[l] = o\n","            callbacks.on_batch_end(batch_index, batch_logs)\n","            if progress_bar is None:\n","                progress_bar = tqdm(total=train_size, desc='Epoch {}/{}'.format(epoch + 1, epochs), unit='samples')\n","            progress_bar.set_postfix(train_on_batch=('[D loss: [%g %g], acc.: [%.2f%% %.2f%%]] [G total_loss: %g, ' +\n","                                                     'img_loss: %g, d_loss: %g, d_acc.: %.2f%%]') %\n","                                                    (d_loss1[0], d_loss2[0], 100 * d_loss1[1], 100 * d_loss2[1],\n","                                                     g_loss[0],\n","                                                     g_loss[0] - g_loss[7], g_loss[7], 100 * g_loss[-1]),\n","                                     augment=augment)\n","            progress_bar.update(len(batch_ids))\n","        MU_gt = [MU_training_val[:, np.array([0])], MU_training_val[:, np.array([1])]]\n","        d_val_loss1 = discriminator.evaluate(MU_gt, hard_valid, batch_size=batch_size, verbose=0)\n","        d_val_loss2 = discriminator.evaluate(generator.predict([PHI_meas_training_val, freq_training_val,\n","                                                                d_training_val])[:2], hard_fake, batch_size=batch_size,\n","                                             verbose=0)\n","        # d_val_loss = discriminator.evaluate([np.concatenate([im1, im2], axis=0)\n","        #                                      for im1, im2 in zip(MU_gt,\n","        #                                                          generator.predict(PHI_meas_training_val)[:2])],\n","        #                                     np.concatenate([hard_valid, hard_fake], axis=0), verbose=0)\n","        d_val_loss = np.zeros(len(discriminator.metrics_names))\n","        for i, l in enumerate(discriminator.metrics_names):\n","            if l in discriminator.stateful_metric_names:\n","                d_val_loss[i] = d_val_loss2[i]\n","            else:\n","                d_val_loss[i] = 0.5 * (d_val_loss1[i] + d_val_loss2[i])\n","        MU_norm_gt = [MU_norm_training_val[:, np.array([0])], MU_norm_training_val[:, np.array([1])]]\n","        progress_bar.set_postfix(validate_on_epoch='[D loss: [%g %g], acc.: [%.2f%% %.2f%%]] validating generator..' %\n","                                                   (d_val_loss1[0], d_val_loss2[0], 100 * d_val_loss1[1],\n","                                                    100 * d_val_loss2[1]))\n","        g_val_loss = gan.evaluate([PHI_meas_training_val, freq_training_val, d_training_val],\n","                                  [MU_gt[0], MU_gt[1], MU_norm_gt[0], MU_norm_gt[1], MUa_val, MUsp_val, hard_valid],\n","                                  batch_size=batch_size, verbose=0)\n","        for l, o in zip(out_labels, np.concatenate([d_val_loss, g_val_loss])):\n","            epoch_logs['val_' + l] = o\n","        # num_batches = (val_size + batch_size - 1) // batch_size\n","        # for batch_index in range(num_batches):\n","        #     batch_start = batch_index * batch_size\n","        #     batch_end = min(val_size, (batch_index + 1) * batch_size)\n","        #     MU_gt = [MU_training_val[batch_start:batch_end, 0], MU_training_val[batch_start:batch_end, 0]]\n","        #     MU_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_gt]\n","        #     d_loss1 = discriminator.test_on_batch(MU_gt, hard_valid[:(batch_end - batch_start)])\n","        #     in_gen = PHI_meas_training_val[batch_start:batch_end]\n","        #     d_loss2 = discriminator.test_on_batch(generator.predict(in_gen)[:2], hard_fake[:(batch_end - batch_start)])\n","        #     d_loss = np.zeros(len(discriminator.metrics_names))\n","        #     for i, l in enumerate(discriminator.metrics_names):\n","        #         if l in discriminator.stateful_metric_names:\n","        #             d_loss[i] = d_loss2[i]\n","        #         else:\n","        #             d_loss[i] = 0.5 * (d_loss1[i] + d_loss2[i])\n","        #     MU_norm_gt = [MU_norm_training_val[batch_start:batch_end, 0],\n","        #                   MU_norm_training_val[batch_start:batch_end, 1]]\n","        #     MU_norm_gt = [im.reshape(im.shape[:1] + (1,) + im.shape[1:]) for im in MU_norm_gt]\n","        #     g_loss = gan.test_on_batch(in_gen, [MU_gt[0], MU_gt[1], MU_norm_gt[0], MU_norm_gt[1],\n","        #                                       MUa_val[batch_start:batch_end], MUsp_val[batch_start:batch_end],\n","        #                                       hard_valid[:(batch_end - batch_start)]])\n","        #     for l, o in zip(out_labels, np.concatenate([d_loss, g_loss])):\n","        #         if l in logger.stateful_metrics:\n","        #             epoch_logs['val_' + l] = o\n","        #         else:\n","        #             if 'val_' + l in epoch_logs:\n","        #                 epoch_logs['val_' + l] += o * (batch_end - batch_start)\n","        #             else:\n","        #                 epoch_logs['val_' + l] = o * (batch_end - batch_start)\n","        #     pbar.set_postfix(validate_on_batch=('[D loss: %g, acc.: %.2f%%] [G total_loss: %g, img_loss: %g, ' +\n","        #                                         'd_loss: %g, d_acc.: %.2f%%]') %\n","        #                                        (d_loss[0], 100 * d_loss[1], g_loss[0], g_loss[0] - g_loss[7],\n","        #                                         g_loss[7], 100 * g_loss[-1]), soft_label=soft_label, augment=augment)\n","        #     pbar.update(batch_end - batch_start)\n","        # for l in out_labels:\n","        #     if l not in logger.stateful_metrics:\n","        #         epoch_logs['val_' + l] /= val_size\n","        callbacks.on_epoch_end(epoch, epoch_logs)\n","        # gan_d_acc[0] = epoch_logs[out_labels[-1]]\n","        # gan_d_acc[1] = epoch_logs['val_' + out_labels[-1]]\n","        progress_bar.set_postfix(train=('[D loss: %g, acc.: %.2f%%] [G total_loss: %g, img_loss: %g, d_loss: %g, ' +\n","                                        'd_acc.: %.2f%%]') %\n","                                       (epoch_logs[out_labels[0]], 100 * epoch_logs[out_labels[1]],\n","                                        epoch_logs[out_labels[2]],\n","                                        epoch_logs[out_labels[2]] - epoch_logs[out_labels[9]],\n","                                        epoch_logs[out_labels[9]],\n","                                        100 * epoch_logs[out_labels[-1]]),\n","                                 validation=('[D loss: %g, acc.: %.2f%%] [G total_loss: %g, img_loss: %g, ' +\n","                                             'd_loss: %g, d_acc.: %.2f%%]') %\n","                                            (epoch_logs['val_' + out_labels[0]],\n","                                             100 * epoch_logs['val_' + out_labels[1]],\n","                                             epoch_logs['val_' + out_labels[2]],\n","                                             epoch_logs['val_' + out_labels[2]] - epoch_logs['val_' + out_labels[9]],\n","                                             epoch_logs['val_' + out_labels[9]],\n","                                             100 * epoch_logs['val_' + out_labels[-1]]))\n","        progress_bar.close()\n","    callbacks.on_train_end()\n","    hist_data = {\n","        'train_start': time_history.train_time_start,\n","        'train_time': time_history.train_time,\n","        'epoch_time': time_history.times,\n","        'history': history.history,\n","        'choice': np.where(choice == 1)[0].tolist()\n","    }\n","    return hist_data\n","\n","\n","hist_data = run_epoch(100)\n","\n","gan.save('model46log_100.h5')\n","generator.save('model46log_100_gen.h5')\n","discriminator.save('model46log_100_dis.h5')\n","with open('model_hist_data46.0log.json', 'w') as f:\n","    json.dump(hist_data, f)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bz_XAArtp6b-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"01c7840d-0157-4f3a-e3a3-094c6948ab33"},"source":["from config_16x15_seq.builder import *\n","\n","\n","lr = 0.0002\n","beta_1 = 0.5\n","\n","\n","model = primary_net()\n","loss = 'mse'\n","metrics = ['mae', 'mse', 'mape', 'msle', 'logcosh', 'cosine']\n","model.compile(loss=loss, loss_weights=[0, 0, 20, 20, 2e+4, 2], optimizer=Adam(lr=lr, beta_1=beta_1), metrics=metrics)\n","model.summary()\n","# checkpoint = callbacks.ModelCheckpoint('model_test{epoch:02d}.h5', verbose=1)\n","callbacks = [TimeHistory()]\n","# callbacks = [TimeHistory(), EarlyStopping(monitor='loss', min_delta=0.1, patience=100,\n","#                                           restore_best_weights=True, verbose=1)]\n","# early_stop = callbacks.EarlyStopping(patience=200, verbose=1)\n","choice = np.zeros(data_size, dtype='uint8')\n","choice[np.random.choice(data_size, val_size, replace=False)] = 1\n","# choice = data_cat\n","x0 = [PHI_meas['training'][choice == 0], freq['training'][choice == 0], d['training'][choice == 0]]\n","y0 = [MU['training'][choice == 0][:, np.array([0])], MU['training'][choice == 0][:, np.array([1])],\n","      MU_norm['training'][choice == 0][:, np.array([0])], MU_norm['training'][choice == 0][:, np.array([1])],\n","      MUa['training'][choice == 0], MUsp['training'][choice == 0]]\n","x1 = [PHI_meas['training'][choice == 1], freq['training'][choice == 1], d['training'][choice == 1]]\n","y1 = [MU['training'][choice == 1][:, np.array([0])], MU['training'][choice == 1][:, np.array([1])],\n","      MU_norm['training'][choice == 1][:, np.array([0])], MU_norm['training'][choice == 1][:, np.array([1])],\n","      MUa['training'][choice == 1], MUsp['training'][choice == 1]]\n","# n = np.where(choice[-N_count[-1]:] == 0)[0].size\n","# w = 1.5\n","# sw1 = np.ones(train_size)\n","# sw2 = np.concatenate([np.ones(train_size-n) * (train_size - n * w) / (train_size - n), np.ones(n) * w])\n","# sample_weight = [sw1, sw1, sw2, sw2, sw1, sw1]\n","history = model.fit(x0, y0, validation_data=(x1, y1), batch_size=32, epochs=100, verbose=2, callbacks=callbacks)\n","hist_data = {\n","    'train_start': callbacks[0].train_time_start,\n","    'train_time': callbacks[0].train_time,\n","    'epoch_time': callbacks[0].times,\n","    'history': history.history,\n","    'choice': np.where(choice == 1)[0].tolist()\n","}\n","\n","# Summarize for accuracy training history\n","# plt.plot(history.history['acc'])\n","# plt.plot(history.history['val_acc'])\n","# plt.title('model accuracy')\n","# plt.ylabel('accuracy')\n","# plt.xlabel('epoch')\n","# plt.legend(['train', 'test'], loc='upper left')\n","# plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /content/drive/My Drive/thesis/config_16x15_seq/helper.py:54: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/thesis/config_16x15_seq/helper.py:56: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, 7)      0                                            \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, None, 6)      0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, 1)            0                                            \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 512)          403968      lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 1)            0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 1)            0           input_3[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 514)          0           bidirectional_1[0][0]            \n","                                                                 lambda_3[0][0]                   \n","                                                                 lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 8192)         4218880     concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","reshape_1 (Reshape)             (None, 2, 64, 64)    0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 4, 64, 64)    76          reshape_1[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 4, 64, 64)    16          conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)       (None, 4, 64, 64)    0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 4, 64, 64)    148         leaky_re_lu_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 4, 64, 64)    16          conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)       (None, 4, 64, 64)    0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 8, 32, 32)    520         leaky_re_lu_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 8, 32, 32)    32          conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 8, 32, 32)    584         leaky_re_lu_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 8, 32, 32)    32          conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 8, 32, 32)    584         leaky_re_lu_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 8, 32, 32)    32          conv2d_5[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)       (None, 8, 32, 32)    0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 16, 16, 16)   2064        leaky_re_lu_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 16, 16, 16)   64          conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 16, 16, 16)   2320        leaky_re_lu_6[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 16, 16, 16)   64          conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 16, 16, 16)   2320        leaky_re_lu_7[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 16, 16, 16)   64          conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 16)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 32, 8, 8)     8224        leaky_re_lu_8[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 32, 8, 8)     128         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)       (None, 32, 8, 8)     0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_9[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 32, 8, 8)     128         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_10[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 32, 8, 8)     128         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 64, 4, 4)     32832       leaky_re_lu_11[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 64, 4, 4)     256         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 64, 4, 4)     36928       leaky_re_lu_12[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 64, 4, 4)     256         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 64, 4, 4)     36928       leaky_re_lu_13[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 64, 4, 4)     256         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)      (None, 64, 4, 4)     0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 32, 8, 8)     32800       leaky_re_lu_14[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 32, 8, 8)     128         conv2d_transpose_1[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 64, 8, 8)     0           leaky_re_lu_11[0][0]             \n","                                                                 leaky_re_lu_15[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 32, 8, 8)     18464       concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 32, 8, 8)     128         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 32, 8, 8)     9248        leaky_re_lu_16[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 32, 8, 8)     128         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)      (None, 32, 8, 8)     0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 16)   8208        leaky_re_lu_17[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 16, 16, 16)   64          conv2d_transpose_2[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_18 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, 32, 16, 16)   0           leaky_re_lu_8[0][0]              \n","                                                                 leaky_re_lu_18[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 16, 16, 16)   4624        concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 16, 16, 16)   64          conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_19 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 16, 16, 16)   2320        leaky_re_lu_19[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 16, 16, 16)   64          conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_20 (LeakyReLU)      (None, 16, 16, 16)   0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_3 (Conv2DTrans (None, 8, 32, 32)    2056        leaky_re_lu_20[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 32, 32)    32          conv2d_transpose_3[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_21 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, 16, 32, 32)   0           leaky_re_lu_5[0][0]              \n","                                                                 leaky_re_lu_21[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 8, 32, 32)    1160        concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 8, 32, 32)    32          conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_22 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 8, 32, 32)    584         leaky_re_lu_22[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 8, 32, 32)    32          conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_23 (LeakyReLU)      (None, 8, 32, 32)    0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_4 (Conv2DTrans (None, 4, 64, 64)    516         leaky_re_lu_23[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 4, 64, 64)    16          conv2d_transpose_4[0][0]         \n","__________________________________________________________________________________________________\n","leaky_re_lu_24 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 8, 64, 64)    0           leaky_re_lu_2[0][0]              \n","                                                                 leaky_re_lu_24[0][0]             \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 4, 64, 64)    292         concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 4, 64, 64)    16          conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_25 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 4, 64, 64)    148         leaky_re_lu_25[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 4, 64, 64)    16          conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","leaky_re_lu_26 (LeakyReLU)      (None, 4, 64, 64)    0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 64)           32960       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 2, 64, 64)    10          leaky_re_lu_26[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_27 (LeakyReLU)      (None, 64)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 64)           32960       concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 2, 64, 64)    0           reshape_1[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            65          leaky_re_lu_27[0][0]             \n","__________________________________________________________________________________________________\n","leaky_re_lu_28 (LeakyReLU)      (None, 64)           0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 2, 64, 64)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 1)            0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 1)            65          leaky_re_lu_28[0][0]             \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 2, 64, 64)    0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","repeat_vector_1 (RepeatVector)  (None, 4096, 1)      0           lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","repeat_vector_2 (RepeatVector)  (None, 4096, 1)      0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 1, 64, 64)    0           lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_2 (Reshape)             (None, 1, 64, 64)    0           repeat_vector_1[0][0]            \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 1, 64, 64)    0           lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","reshape_3 (Reshape)             (None, 1, 64, 64)    0           repeat_vector_2[0][0]            \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, 1, 64, 64)    0           lambda_7[0][0]                   \n","                                                                 reshape_2[0][0]                  \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, 1, 64, 64)    0           lambda_8[0][0]                   \n","                                                                 reshape_3[0][0]                  \n","==================================================================================================\n","Total params: 4,913,544\n","Trainable params: 4,912,448\n","Non-trainable params: 1,096\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Train on 8000 samples, validate on 2000 samples\n","Epoch 1/100\n"," - 442s - loss: 33.4836 - multiply_1_loss: 3.2282e-04 - multiply_2_loss: 3.2298 - lambda_7_loss: 0.8055 - lambda_8_loss: 0.8069 - lambda_9_loss: 3.3689e-05 - dense_5_loss: 0.2800 - multiply_1_mean_absolute_error: 0.0074 - multiply_1_mean_squared_error: 3.2282e-04 - multiply_1_mean_absolute_percentage_error: 37.8302 - multiply_1_mean_squared_logarithmic_error: 2.8001e-04 - multiply_1_logcosh: 1.6101e-04 - multiply_1_cosine_proximity: -9.3579e-01 - multiply_2_mean_absolute_error: 0.7305 - multiply_2_mean_squared_error: 3.2298 - multiply_2_mean_absolute_percentage_error: 35.3477 - multiply_2_mean_squared_logarithmic_error: 0.1069 - multiply_2_logcosh: 0.4268 - multiply_2_cosine_proximity: -9.3460e-01 - lambda_7_mean_absolute_error: 0.3219 - lambda_7_mean_squared_error: 0.8055 - lambda_7_mean_absolute_percentage_error: 18.7881 - lambda_7_mean_squared_logarithmic_error: 0.0550 - lambda_7_logcosh: 0.1597 - lambda_7_cosine_proximity: -9.3579e-01 - lambda_8_mean_absolute_error: 0.3307 - lambda_8_mean_squared_error: 0.8069 - lambda_8_mean_absolute_percentage_error: 19.7060 - lambda_8_mean_squared_logarithmic_error: 0.0558 - lambda_8_logcosh: 0.1623 - lambda_8_cosine_proximity: -9.3460e-01 - lambda_9_mean_absolute_error: 0.0047 - lambda_9_mean_squared_error: 3.3689e-05 - lambda_9_mean_absolute_percentage_error: 34.8583 - lambda_9_mean_squared_logarithmic_error: 3.2524e-05 - lambda_9_logcosh: 1.6842e-05 - lambda_9_cosine_proximity: -1.0000e+00 - dense_5_mean_absolute_error: 0.4255 - dense_5_mean_squared_error: 0.2800 - dense_5_mean_absolute_percentage_error: 30.1859 - dense_5_mean_squared_logarithmic_error: 0.0408 - dense_5_logcosh: 0.1250 - dense_5_cosine_proximity: -1.0000e+00 - val_loss: 32.5003 - val_multiply_1_loss: 3.1698e-04 - val_multiply_2_loss: 3.1353 - val_lambda_7_loss: 0.7853 - val_lambda_8_loss: 0.7880 - val_lambda_9_loss: 2.9174e-05 - val_dense_5_loss: 0.2250 - val_multiply_1_mean_absolute_error: 0.0078 - val_multiply_1_mean_squared_error: 3.1698e-04 - val_multiply_1_mean_absolute_percentage_error: 41.9847 - val_multiply_1_mean_squared_logarithmic_error: 2.7432e-04 - val_multiply_1_logcosh: 1.5812e-04 - val_multiply_1_cosine_proximity: -9.4113e-01 - val_multiply_2_mean_absolute_error: 0.6934 - val_multiply_2_mean_squared_error: 3.1353 - val_multiply_2_mean_absolute_percentage_error: 32.3905 - val_multiply_2_mean_squared_logarithmic_error: 0.0923 - val_multiply_2_logcosh: 0.4001 - val_multiply_2_cosine_proximity: -9.4073e-01 - val_lambda_7_mean_absolute_error: 0.3329 - val_lambda_7_mean_squared_error: 0.7853 - val_lambda_7_mean_absolute_percentage_error: 20.0721 - val_lambda_7_mean_squared_logarithmic_error: 0.0524 - val_lambda_7_logcosh: 0.1598 - val_lambda_7_cosine_proximity: -9.4113e-01 - val_lambda_8_mean_absolute_error: 0.3255 - val_lambda_8_mean_squared_error: 0.7880 - val_lambda_8_mean_absolute_percentage_error: 19.3771 - val_lambda_8_mean_squared_logarithmic_error: 0.0519 - val_lambda_8_logcosh: 0.1577 - val_lambda_8_cosine_proximity: -9.4073e-01 - val_lambda_9_mean_absolute_error: 0.0045 - val_lambda_9_mean_squared_error: 2.9174e-05 - val_lambda_9_mean_absolute_percentage_error: 34.5060 - val_lambda_9_mean_squared_logarithmic_error: 2.8185e-05 - val_lambda_9_logcosh: 1.4585e-05 - val_lambda_9_cosine_proximity: -1.0000e+00 - val_dense_5_mean_absolute_error: 0.3849 - val_dense_5_mean_squared_error: 0.2250 - val_dense_5_mean_absolute_percentage_error: 26.3573 - val_dense_5_mean_squared_logarithmic_error: 0.0316 - val_dense_5_logcosh: 0.1030 - val_dense_5_cosine_proximity: -1.0000e+00\n","Epoch 2/100\n"," - 433s - loss: 32.3023 - multiply_1_loss: 3.0922e-04 - multiply_2_loss: 3.1007 - lambda_7_loss: 0.7796 - lambda_8_loss: 0.7827 - lambda_9_loss: 2.9576e-05 - dense_5_loss: 0.2325 - multiply_1_mean_absolute_error: 0.0073 - multiply_1_mean_squared_error: 3.0922e-04 - multiply_1_mean_absolute_percentage_error: 37.6126 - multiply_1_mean_squared_logarithmic_error: 2.6751e-04 - multiply_1_logcosh: 1.5424e-04 - multiply_1_cosine_proximity: -9.4265e-01 - multiply_2_mean_absolute_error: 0.7064 - multiply_2_mean_squared_error: 3.1007 - multiply_2_mean_absolute_percentage_error: 34.1170 - multiply_2_mean_squared_logarithmic_error: 0.0949 - multiply_2_logcosh: 0.4082 - multiply_2_cosine_proximity: -9.4214e-01 - lambda_7_mean_absolute_error: 0.3252 - lambda_7_mean_squared_error: 0.7796 - lambda_7_mean_absolute_percentage_error: 19.3989 - lambda_7_mean_squared_logarithmic_error: 0.0515 - lambda_7_logcosh: 0.1570 - lambda_7_cosine_proximity: -9.4265e-01 - lambda_8_mean_absolute_error: 0.3267 - lambda_8_mean_squared_error: 0.7827 - lambda_8_mean_absolute_percentage_error: 19.5030 - lambda_8_mean_squared_logarithmic_error: 0.0518 - lambda_8_logcosh: 0.1581 - lambda_8_cosine_proximity: -9.4214e-01 - lambda_9_mean_absolute_error: 0.0044 - lambda_9_mean_squared_error: 2.9576e-05 - lambda_9_mean_absolute_percentage_error: 32.0872 - lambda_9_mean_squared_logarithmic_error: 2.8539e-05 - lambda_9_logcosh: 1.4786e-05 - lambda_9_cosine_proximity: -1.0000e+00 - dense_5_mean_absolute_error: 0.3908 - dense_5_mean_squared_error: 0.2325 - dense_5_mean_absolute_percentage_error: 27.4119 - dense_5_mean_squared_logarithmic_error: 0.0329 - dense_5_logcosh: 0.1063 - dense_5_cosine_proximity: -1.0000e+00 - val_loss: 32.1771 - val_multiply_1_loss: 3.1063e-04 - val_multiply_2_loss: 3.0835 - val_lambda_7_loss: 0.7813 - val_lambda_8_loss: 0.7828 - val_lambda_9_loss: 2.5375e-05 - val_dense_5_loss: 0.1934 - val_multiply_1_mean_absolute_error: 0.0074 - val_multiply_1_mean_squared_error: 3.1063e-04 - val_multiply_1_mean_absolute_percentage_error: 38.7697 - val_multiply_1_mean_squared_logarithmic_error: 2.6818e-04 - val_multiply_1_logcosh: 1.5495e-04 - val_multiply_1_cosine_proximity: -9.4203e-01 - val_multiply_2_mean_absolute_error: 0.6979 - val_multiply_2_mean_squared_error: 3.0835 - val_multiply_2_mean_absolute_percentage_error: 32.7044 - val_multiply_2_mean_squared_logarithmic_error: 0.0903 - val_multiply_2_logcosh: 0.4030 - val_multiply_2_cosine_proximity: -9.4200e-01 - val_lambda_7_mean_absolute_error: 0.3374 - val_lambda_7_mean_squared_error: 0.7813 - val_lambda_7_mean_absolute_percentage_error: 20.6110 - val_lambda_7_mean_squared_logarithmic_error: 0.0523 - val_lambda_7_logcosh: 0.1606 - val_lambda_7_cosine_proximity: -9.4203e-01 - val_lambda_8_mean_absolute_error: 0.3352 - val_lambda_8_mean_squared_error: 0.7828 - val_lambda_8_mean_absolute_percentage_error: 20.4972 - val_lambda_8_mean_squared_logarithmic_error: 0.0521 - val_lambda_8_logcosh: 0.1598 - val_lambda_8_cosine_proximity: -9.4200e-01 - val_lambda_9_mean_absolute_error: 0.0042 - val_lambda_9_mean_squared_error: 2.5375e-05 - val_lambda_9_mean_absolute_percentage_error: 31.1452 - val_lambda_9_mean_squared_logarithmic_error: 2.4494e-05 - val_lambda_9_logcosh: 1.2685e-05 - val_lambda_9_cosine_proximity: -1.0000e+00 - val_dense_5_mean_absolute_error: 0.3594 - val_dense_5_mean_squared_error: 0.1934 - val_dense_5_mean_absolute_percentage_error: 24.7922 - val_dense_5_mean_squared_logarithmic_error: 0.0273 - val_dense_5_logcosh: 0.0898 - val_dense_5_cosine_proximity: -1.0000e+00\n","Epoch 3/100\n"," - 432s - loss: 31.8423 - multiply_1_loss: 3.0195e-04 - multiply_2_loss: 3.0471 - lambda_7_loss: 0.7713 - lambda_8_loss: 0.7747 - lambda_9_loss: 2.6007e-05 - dense_5_loss: 0.2019 - multiply_1_mean_absolute_error: 0.0071 - multiply_1_mean_squared_error: 3.0195e-04 - multiply_1_mean_absolute_percentage_error: 35.6642 - multiply_1_mean_squared_logarithmic_error: 2.6098e-04 - multiply_1_logcosh: 1.5063e-04 - multiply_1_cosine_proximity: -9.4146e-01 - multiply_2_mean_absolute_error: 0.6937 - multiply_2_mean_squared_error: 3.0471 - multiply_2_mean_absolute_percentage_error: 32.5588 - multiply_2_mean_squared_logarithmic_error: 0.0914 - multiply_2_logcosh: 0.4045 - multiply_2_cosine_proximity: -9.4106e-01 - lambda_7_mean_absolute_error: 0.3268 - lambda_7_mean_squared_error: 0.7713 - lambda_7_mean_absolute_percentage_error: 19.7614 - lambda_7_mean_squared_logarithmic_error: 0.0515 - lambda_7_logcosh: 0.1586 - lambda_7_cosine_proximity: -9.4146e-01 - lambda_8_mean_absolute_error: 0.3288 - lambda_8_mean_squared_error: 0.7747 - lambda_8_mean_absolute_percentage_error: 19.9337 - lambda_8_mean_squared_logarithmic_error: 0.0518 - lambda_8_logcosh: 0.1597 - lambda_8_cosine_proximity: -9.4106e-01 - lambda_9_mean_absolute_error: 0.0042 - lambda_9_mean_squared_error: 2.6007e-05 - lambda_9_mean_absolute_percentage_error: 29.3740 - lambda_9_mean_squared_logarithmic_error: 2.5079e-05 - lambda_9_logcosh: 1.3001e-05 - lambda_9_cosine_proximity: -1.0000e+00 - dense_5_mean_absolute_error: 0.3613 - dense_5_mean_squared_error: 0.2019 - dense_5_mean_absolute_percentage_error: 25.0447 - dense_5_mean_squared_logarithmic_error: 0.0282 - dense_5_logcosh: 0.0930 - dense_5_cosine_proximity: -1.0000e+00 - val_loss: 43.3720 - val_multiply_1_loss: 3.6647e-04 - val_multiply_2_loss: 3.9789 - val_lambda_7_loss: 1.0971 - val_lambda_8_loss: 1.0366 - val_lambda_9_loss: 2.1618e-05 - val_dense_5_loss: 0.1325 - val_multiply_1_mean_absolute_error: 0.0089 - val_multiply_1_mean_squared_error: 3.6647e-04 - val_multiply_1_mean_absolute_percentage_error: 47.6523 - val_multiply_1_mean_squared_logarithmic_error: 3.2028e-04 - val_multiply_1_logcosh: 1.8292e-04 - val_multiply_1_cosine_proximity: -9.2601e-01 - val_multiply_2_mean_absolute_error: 0.9925 - val_multiply_2_mean_squared_error: 3.9789 - val_multiply_2_mean_absolute_percentage_error: 53.9165 - val_multiply_2_mean_squared_logarithmic_error: 0.1428 - val_multiply_2_logcosh: 0.6469 - val_multiply_2_cosine_proximity: -9.2690e-01 - val_lambda_7_mean_absolute_error: 0.5422 - val_lambda_7_mean_squared_error: 1.0971 - val_lambda_7_mean_absolute_percentage_error: 43.5195 - val_lambda_7_mean_squared_logarithmic_error: 0.0934 - val_lambda_7_logcosh: 0.2833 - val_lambda_7_cosine_proximity: -9.2601e-01 - val_lambda_8_mean_absolute_error: 0.5095 - val_lambda_8_mean_squared_error: 1.0366 - val_lambda_8_mean_absolute_percentage_error: 40.0713 - val_lambda_8_mean_squared_logarithmic_error: 0.0867 - val_lambda_8_logcosh: 0.2648 - val_lambda_8_cosine_proximity: -9.2690e-01 - val_lambda_9_mean_absolute_error: 0.0036 - val_lambda_9_mean_squared_error: 2.1618e-05 - val_lambda_9_mean_absolute_percentage_error: 21.8371 - val_lambda_9_mean_squared_logarithmic_error: 2.0789e-05 - val_lambda_9_logcosh: 1.0806e-05 - val_lambda_9_cosine_proximity: -1.0000e+00 - val_dense_5_mean_absolute_error: 0.3015 - val_dense_5_mean_squared_error: 0.1325 - val_dense_5_mean_absolute_percentage_error: 21.6545 - val_dense_5_mean_squared_logarithmic_error: 0.0193 - val_dense_5_logcosh: 0.0631 - val_dense_5_cosine_proximity: -1.0000e+00\n","Epoch 4/100\n"," - 433s - loss: 29.9727 - multiply_1_loss: 2.7908e-04 - multiply_2_loss: 2.8707 - lambda_7_loss: 0.7271 - lambda_8_loss: 0.7373 - lambda_9_loss: 1.9194e-05 - dense_5_loss: 0.1504 - multiply_1_mean_absolute_error: 0.0068 - multiply_1_mean_squared_error: 2.7908e-04 - multiply_1_mean_absolute_percentage_error: 31.7779 - multiply_1_mean_squared_logarithmic_error: 2.4065e-04 - multiply_1_logcosh: 1.3924e-04 - multiply_1_cosine_proximity: -9.4196e-01 - multiply_2_mean_absolute_error: 0.6593 - multiply_2_mean_squared_error: 2.8707 - multiply_2_mean_absolute_percentage_error: 29.5952 - multiply_2_mean_squared_logarithmic_error: 0.0826 - multiply_2_logcosh: 0.3888 - multiply_2_cosine_proximity: -9.4206e-01 - lambda_7_mean_absolute_error: 0.3183 - lambda_7_mean_squared_error: 0.7271 - lambda_7_mean_absolute_percentage_error: 19.7522 - lambda_7_mean_squared_logarithmic_error: 0.0492 - lambda_7_logcosh: 0.1583 - lambda_7_cosine_proximity: -9.4196e-01 - lambda_8_mean_absolute_error: 0.3191 - lambda_8_mean_squared_error: 0.7373 - lambda_8_mean_absolute_percentage_error: 19.6482 - lambda_8_mean_squared_logarithmic_error: 0.0496 - lambda_8_logcosh: 0.1586 - lambda_8_cosine_proximity: -9.4206e-01 - lambda_9_mean_absolute_error: 0.0035 - lambda_9_mean_squared_error: 1.9194e-05 - lambda_9_mean_absolute_percentage_error: 23.7734 - lambda_9_mean_squared_logarithmic_error: 1.8488e-05 - lambda_9_logcosh: 9.5953e-06 - lambda_9_cosine_proximity: -1.0000e+00 - dense_5_mean_absolute_error: 0.3085 - dense_5_mean_squared_error: 0.1504 - dense_5_mean_absolute_percentage_error: 20.8842 - dense_5_mean_squared_logarithmic_error: 0.0207 - dense_5_logcosh: 0.0704 - dense_5_cosine_proximity: -1.0000e+00 - val_loss: 50.4537 - val_multiply_1_loss: 5.6873e-04 - val_multiply_2_loss: 3.8011 - val_lambda_7_loss: 1.2779 - val_lambda_8_loss: 1.2195 - val_lambda_9_loss: 1.3437e-05 - val_dense_5_loss: 0.1179 - val_multiply_1_mean_absolute_error: 0.0119 - val_multiply_1_mean_squared_error: 5.6873e-04 - val_multiply_1_mean_absolute_percentage_error: 65.9515 - val_multiply_1_mean_squared_logarithmic_error: 4.9707e-04 - val_multiply_1_logcosh: 2.8391e-04 - val_multiply_1_cosine_proximity: -9.2723e-01 - val_multiply_2_mean_absolute_error: 0.8278 - val_multiply_2_mean_squared_error: 3.8011 - val_multiply_2_mean_absolute_percentage_error: 38.2800 - val_multiply_2_mean_squared_logarithmic_error: 0.1149 - val_multiply_2_logcosh: 0.5600 - val_multiply_2_cosine_proximity: -9.2971e-01 - val_lambda_7_mean_absolute_error: 0.5376 - val_lambda_7_mean_squared_error: 1.2779 - val_lambda_7_mean_absolute_percentage_error: 43.9514 - val_lambda_7_mean_squared_logarithmic_error: 0.1029 - val_lambda_7_logcosh: 0.3131 - val_lambda_7_cosine_proximity: -9.2723e-01 - val_lambda_8_mean_absolute_error: 0.5310 - val_lambda_8_mean_squared_error: 1.2195 - val_lambda_8_mean_absolute_percentage_error: 43.1387 - val_lambda_8_mean_squared_logarithmic_error: 0.0993 - val_lambda_8_logcosh: 0.3030 - val_lambda_8_cosine_proximity: -9.2971e-01 - val_lambda_9_mean_absolute_error: 0.0031 - val_lambda_9_mean_squared_error: 1.3437e-05 - val_lambda_9_mean_absolute_percentage_error: 24.0547 - val_lambda_9_mean_squared_logarithmic_error: 1.2971e-05 - val_lambda_9_logcosh: 6.7169e-06 - val_lambda_9_cosine_proximity: -1.0000e+00 - val_dense_5_mean_absolute_error: 0.2664 - val_dense_5_mean_squared_error: 0.1179 - val_dense_5_mean_absolute_percentage_error: 14.2020 - val_dense_5_mean_squared_logarithmic_error: 0.0131 - val_dense_5_logcosh: 0.0559 - val_dense_5_cosine_proximity: -1.0000e+00\n","Epoch 5/100\n"],"name":"stdout"}]}]}